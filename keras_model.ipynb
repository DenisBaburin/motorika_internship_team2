{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import optuna\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to read OMG data from a CSV file\n",
    "def read_omg_csv(path_palm_data: str, \n",
    "                 n_omg_channels: int, \n",
    "                 n_acc_channels: int = 0, \n",
    "                 n_gyr_channels: int = 0, \n",
    "                 n_mag_channels: int = 0, \n",
    "                 n_enc_channels: int = 0,\n",
    "                 button_ch: bool = True, \n",
    "                 sync_ch: bool = True, \n",
    "                 timestamp_ch: bool = True) -> pd.DataFrame:\n",
    "    \n",
    "    df_raw = pd.read_csv(path_palm_data, sep=' ', \n",
    "                         header=None, \n",
    "                         skipfooter=1, \n",
    "                         skiprows=1, \n",
    "                         engine='python')\n",
    "    columns = np.arange(n_omg_channels).astype('str').tolist()\n",
    "    \n",
    "    for label, label_count in zip(['ACC', 'GYR', 'MAG', 'ENC'], \n",
    "                                  [n_acc_channels, n_gyr_channels, n_mag_channels, n_enc_channels]):\n",
    "        columns = columns + ['{}{}'.format(label, i) for i in range(label_count)]\n",
    "        \n",
    "    if button_ch:\n",
    "        columns = columns + ['BUTTON']\n",
    "        \n",
    "    if sync_ch:\n",
    "        columns = columns + ['SYNC']\n",
    "        \n",
    "    if timestamp_ch:\n",
    "        columns = columns + ['ts']\n",
    "        \n",
    "    df_raw.columns = columns\n",
    "    \n",
    "    return df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15679, 106), (15679,), (3889, 106), (3889,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_training_data(path_palm_data, path_protocol_data, path_meta_data, \n",
    "                          n_omg_channels=50, n_acc_channels=3, n_gyr_channels=3, \n",
    "                          n_mag_channels=0, n_enc_channels=6, \n",
    "                          standardize=True, normalize=True,\n",
    "                          DO_REPLACE_TO_MOVING_AVERAGE=True, \n",
    "                          DO_CALCULATE_DERIVATIVE=True,\n",
    "                          DO_SHIFT_GESTURE=True,\n",
    "                          selected_channels='ALL'):\n",
    "    \"\"\"\n",
    "    Подготовка данных для обучения и тестирования из файлов данных palm, protocol и meta.\n",
    "    \n",
    "    Аргументы:\n",
    "    path_palm_data (str): Путь к файлу данных palm.\n",
    "    path_protocol_data (str): Путь к файлу данных protocol.\n",
    "    path_meta_data (str): Путь к файлу данных meta.\n",
    "    n_omg_channels, n_acc_channels, и т.д. (int): Количество каналов сенсоров.\n",
    "    standardize (bool): Если True, стандартизирует признаки.\n",
    "    normalize (bool): Если True, нормализует признаки.\n",
    "    DO_REPLACE_TO_MOVING_AVERAGE (bool): Если True, применяет скользящее среднее к данным OMG.\n",
    "    DO_CALCULATE_DERIVATIVE (bool): Если True, вычисляет производные данных OMG.\n",
    "    DO_SHIFT_GESTURE (bool): Если True, смещает целевой признак на максимальный скачок в данных.\n",
    "    selected_channels (str): Выбор каналов данных ('OMG', 'ACC_GYR', 'ALL').\n",
    "    \n",
    "    Возвращает:\n",
    "    tuple: Кортеж, содержащий данные для обучения и тестирования.\n",
    "    \"\"\"\n",
    "    # Чтение данных OMG\n",
    "    omg_data = read_omg_csv(path_palm_data, n_omg_channels, n_acc_channels, n_gyr_channels, \n",
    "                            n_mag_channels, n_enc_channels)\n",
    "    \n",
    "    # Чтение данных протокола и кодирование жестов\n",
    "    gestures_protocol = pd.read_csv(path_protocol_data)\n",
    "    le = LabelEncoder()\n",
    "    gestures_protocol['gesture'] = le.fit_transform(\n",
    "        gestures_protocol[[\n",
    "            \"Thumb\", \"Index\", \"Middle\", \"Ring\", \"Pinky\",\n",
    "            'Thumb_stretch', 'Index_stretch', 'Middle_stretch', 'Ring_stretch', 'Pinky_stretch'\n",
    "        ]].apply(lambda row: str(tuple(row)), axis=1)\n",
    "    )\n",
    "    \n",
    "    # Чтение метаинформации\n",
    "    df_meta = pd.read_csv(path_meta_data)\n",
    "    palm_file = path_palm_data.split('/')[-1]\n",
    "    last_train_idx = df_meta[df_meta['montage'] == palm_file].to_dict(orient='records')[0]['last_train_idx']\n",
    "    \n",
    "    # Синхронизация меток жестов с данными OMG, используя канал SYNC\n",
    "    y_cmd = np.array([gestures_protocol['gesture'].loc[s] for s in omg_data['SYNC'].values])\n",
    "    \n",
    "    # Подготовка названий признаков для данных OMG\n",
    "    OMG_CH = [str(i) for i in range(n_omg_channels)]\n",
    "    ACC_CH = ['ACC0', 'ACC1', 'ACC2']\n",
    "    GYR_CH = ['GYR0', 'GYR1', 'GYR2']\n",
    "    ALL_CH = OMG_CH + ACC_CH + GYR_CH\n",
    "\n",
    "    # Выбор каналов в соответствии с параметром selected_channels\n",
    "    if selected_channels == 'OMG':\n",
    "        selected_features = OMG_CH\n",
    "    elif selected_channels == 'ACC_GYR':\n",
    "        selected_features = ACC_CH + GYR_CH\n",
    "    else:\n",
    "        selected_features = ALL_CH\n",
    "    \n",
    "    if DO_REPLACE_TO_MOVING_AVERAGE:\n",
    "        # Замена на скользящее среднее\n",
    "        for col in selected_features:\n",
    "            omg_data[col] = omg_data[col].rolling(window=5).mean().bfill()\n",
    "    \n",
    "    if DO_CALCULATE_DERIVATIVE:\n",
    "        # Вычисление производных данных\n",
    "        OMG_DERIV = [f'{col}_deriv' for col in OMG_CH]\n",
    "        for col in OMG_CH:\n",
    "            omg_data[f'{col}_next'] = omg_data[col].shift(-1).ffill()\n",
    "            omg_data[f'{col}_deriv'] = omg_data[f'{col}_next'] - omg_data[col]\n",
    "        selected_features += OMG_DERIV\n",
    "\n",
    "    if DO_SHIFT_GESTURE:\n",
    "        # Смещение целевого признака\n",
    "        id_max = 0\n",
    "        cur_gesture = 0\n",
    "        for i in range(y_cmd.shape[0]):\n",
    "            if i < id_max:  # Пропускаем все значения до id_max\n",
    "                continue\n",
    "            prev_gesture = cur_gesture  # предыдущий жест\n",
    "            cur_gesture = y_cmd[i]  # текущий жест\n",
    "            if cur_gesture != prev_gesture:  # Если сменился жест\n",
    "                id_max = omg_data[OMG_DERIV][i:i+35].abs().sum(axis=1).idxmax()  # Нахождение максимального скачка\n",
    "                y_cmd[i:id_max] = prev_gesture  # Замена всех значений до id_max на предыдущий жест\n",
    "    \n",
    "    # Разделение данных на обучающие и тестовые наборы\n",
    "    X_train = omg_data[selected_features].iloc[:last_train_idx].values\n",
    "    y_train = y_cmd[:last_train_idx]\n",
    "    X_test = omg_data[selected_features].iloc[last_train_idx:].values\n",
    "    y_test = y_cmd[last_train_idx:]\n",
    "    \n",
    "    # Стандартизация и нормализация\n",
    "    if standardize:\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        \n",
    "    if normalize:\n",
    "        scaler = MinMaxScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "    \n",
    "    return (X_train, y_train), (X_test, y_test)\n",
    "\n",
    "path_palm_data = 'data/2023-05-31_17-14-41.palm'\n",
    "path_protocol_data = 'data/2023-05-31_17-14-41.palm.protocol.csv'\n",
    "path_meta_data = 'data/meta_information.csv'\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = prepare_training_data(path_palm_data, path_protocol_data, path_meta_data, standardize=True, normalize=False)\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4393 - loss: 1.7728 - val_accuracy: 0.8195 - val_loss: 0.5047\n",
      "Epoch 2/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7979 - loss: 0.6538 - val_accuracy: 0.8785 - val_loss: 0.3265\n",
      "Epoch 3/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8500 - loss: 0.4584 - val_accuracy: 0.9043 - val_loss: 0.2397\n",
      "Epoch 4/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8750 - loss: 0.3696 - val_accuracy: 0.9225 - val_loss: 0.2024\n",
      "Epoch 5/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8955 - loss: 0.3163 - val_accuracy: 0.9381 - val_loss: 0.1650\n",
      "Epoch 6/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9119 - loss: 0.2501 - val_accuracy: 0.9436 - val_loss: 0.1447\n",
      "Epoch 7/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9215 - loss: 0.2275 - val_accuracy: 0.9496 - val_loss: 0.1360\n",
      "Epoch 8/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9358 - loss: 0.1884 - val_accuracy: 0.9496 - val_loss: 0.1347\n",
      "Epoch 9/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9389 - loss: 0.1729 - val_accuracy: 0.9640 - val_loss: 0.1080\n",
      "Epoch 10/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9429 - loss: 0.1597 - val_accuracy: 0.9570 - val_loss: 0.1185\n",
      "Epoch 11/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9541 - loss: 0.1310 - val_accuracy: 0.9595 - val_loss: 0.1092\n",
      "Epoch 12/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9603 - loss: 0.1184 - val_accuracy: 0.9611 - val_loss: 0.1054\n",
      "Epoch 13/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9560 - loss: 0.1212 - val_accuracy: 0.9665 - val_loss: 0.0966\n",
      "Epoch 14/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9634 - loss: 0.1002 - val_accuracy: 0.9627 - val_loss: 0.1066\n",
      "Epoch 15/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9653 - loss: 0.1084 - val_accuracy: 0.9621 - val_loss: 0.1034\n",
      "Epoch 16/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9666 - loss: 0.0949 - val_accuracy: 0.9633 - val_loss: 0.1099\n",
      "Epoch 17/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9684 - loss: 0.0916 - val_accuracy: 0.9627 - val_loss: 0.1083\n",
      "Epoch 18/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9705 - loss: 0.0919 - val_accuracy: 0.9633 - val_loss: 0.1066\n",
      "Epoch 19/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9682 - loss: 0.0834 - val_accuracy: 0.9592 - val_loss: 0.1131\n",
      "Epoch 20/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9709 - loss: 0.0814 - val_accuracy: 0.9617 - val_loss: 0.1159\n",
      "Epoch 21/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9724 - loss: 0.0796 - val_accuracy: 0.9675 - val_loss: 0.1004\n",
      "Epoch 22/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9755 - loss: 0.0723 - val_accuracy: 0.9662 - val_loss: 0.0998\n",
      "Epoch 23/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9767 - loss: 0.0696 - val_accuracy: 0.9643 - val_loss: 0.1041\n",
      "Epoch 24/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9763 - loss: 0.0688 - val_accuracy: 0.9643 - val_loss: 0.1001\n",
      "Epoch 25/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9786 - loss: 0.0603 - val_accuracy: 0.9707 - val_loss: 0.0933\n",
      "Epoch 26/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9768 - loss: 0.0650 - val_accuracy: 0.9732 - val_loss: 0.0875\n",
      "Epoch 27/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9797 - loss: 0.0609 - val_accuracy: 0.9703 - val_loss: 0.0920\n",
      "Epoch 28/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9819 - loss: 0.0582 - val_accuracy: 0.9700 - val_loss: 0.0961\n",
      "Epoch 29/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9786 - loss: 0.0586 - val_accuracy: 0.9636 - val_loss: 0.1115\n",
      "Epoch 30/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9816 - loss: 0.0562 - val_accuracy: 0.9694 - val_loss: 0.0956\n",
      "Epoch 31/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9813 - loss: 0.0531 - val_accuracy: 0.9633 - val_loss: 0.1122\n",
      "Epoch 32/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9826 - loss: 0.0536 - val_accuracy: 0.9659 - val_loss: 0.1075\n",
      "Epoch 33/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9832 - loss: 0.0520 - val_accuracy: 0.9697 - val_loss: 0.0975\n",
      "Epoch 34/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9830 - loss: 0.0461 - val_accuracy: 0.9665 - val_loss: 0.1066\n",
      "Epoch 35/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9867 - loss: 0.0428 - val_accuracy: 0.9703 - val_loss: 0.1043\n",
      "Epoch 36/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9832 - loss: 0.0457 - val_accuracy: 0.9694 - val_loss: 0.1071\n",
      "Epoch 37/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9855 - loss: 0.0432 - val_accuracy: 0.9710 - val_loss: 0.0984\n",
      "Epoch 38/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9853 - loss: 0.0441 - val_accuracy: 0.9764 - val_loss: 0.0850\n",
      "Epoch 39/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9795 - loss: 0.0535 - val_accuracy: 0.9697 - val_loss: 0.1049\n",
      "Epoch 40/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9854 - loss: 0.0438 - val_accuracy: 0.9675 - val_loss: 0.1083\n",
      "Epoch 41/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9825 - loss: 0.0478 - val_accuracy: 0.9630 - val_loss: 0.1155\n",
      "Epoch 42/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9854 - loss: 0.0434 - val_accuracy: 0.9665 - val_loss: 0.1131\n",
      "Epoch 43/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9844 - loss: 0.0448 - val_accuracy: 0.9732 - val_loss: 0.0963\n",
      "Epoch 44/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9851 - loss: 0.0423 - val_accuracy: 0.9735 - val_loss: 0.0870\n",
      "Epoch 45/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9868 - loss: 0.0414 - val_accuracy: 0.9713 - val_loss: 0.0999\n",
      "Epoch 46/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9885 - loss: 0.0345 - val_accuracy: 0.9713 - val_loss: 0.0993\n",
      "Epoch 47/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9856 - loss: 0.0384 - val_accuracy: 0.9694 - val_loss: 0.1078\n",
      "Epoch 48/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9889 - loss: 0.0336 - val_accuracy: 0.9678 - val_loss: 0.1154\n",
      "Epoch 49/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9856 - loss: 0.0434 - val_accuracy: 0.9675 - val_loss: 0.1178\n",
      "Epoch 50/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9829 - loss: 0.0465 - val_accuracy: 0.9703 - val_loss: 0.1020\n",
      "Epoch 51/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9871 - loss: 0.0357 - val_accuracy: 0.9672 - val_loss: 0.1179\n",
      "Epoch 52/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9894 - loss: 0.0343 - val_accuracy: 0.9675 - val_loss: 0.1128\n",
      "Epoch 53/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9882 - loss: 0.0335 - val_accuracy: 0.9691 - val_loss: 0.1087\n",
      "Epoch 54/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9864 - loss: 0.0402 - val_accuracy: 0.9656 - val_loss: 0.1313\n",
      "Epoch 55/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9881 - loss: 0.0333 - val_accuracy: 0.9681 - val_loss: 0.1152\n",
      "Epoch 56/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9876 - loss: 0.0355 - val_accuracy: 0.9681 - val_loss: 0.1154\n",
      "Epoch 57/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9903 - loss: 0.0288 - val_accuracy: 0.9688 - val_loss: 0.1147\n",
      "Epoch 58/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9894 - loss: 0.0301 - val_accuracy: 0.9707 - val_loss: 0.1107\n",
      "Epoch 59/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9883 - loss: 0.0343 - val_accuracy: 0.9723 - val_loss: 0.1101\n",
      "Epoch 60/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9895 - loss: 0.0287 - val_accuracy: 0.9707 - val_loss: 0.1106\n",
      "Epoch 61/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9895 - loss: 0.0301 - val_accuracy: 0.9684 - val_loss: 0.1156\n",
      "Epoch 62/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9890 - loss: 0.0296 - val_accuracy: 0.9665 - val_loss: 0.1275\n",
      "Epoch 63/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9895 - loss: 0.0297 - val_accuracy: 0.9668 - val_loss: 0.1241\n",
      "Epoch 64/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9907 - loss: 0.0290 - val_accuracy: 0.9707 - val_loss: 0.1157\n",
      "Epoch 65/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9907 - loss: 0.0290 - val_accuracy: 0.9697 - val_loss: 0.1157\n",
      "Epoch 66/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9896 - loss: 0.0320 - val_accuracy: 0.9621 - val_loss: 0.1405\n",
      "Epoch 67/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9871 - loss: 0.0375 - val_accuracy: 0.9640 - val_loss: 0.1373\n",
      "Epoch 68/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9888 - loss: 0.0336 - val_accuracy: 0.9659 - val_loss: 0.1324\n",
      "Epoch 69/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9902 - loss: 0.0307 - val_accuracy: 0.9636 - val_loss: 0.1356\n",
      "Epoch 70/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9880 - loss: 0.0321 - val_accuracy: 0.9684 - val_loss: 0.1308\n",
      "Epoch 71/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9889 - loss: 0.0299 - val_accuracy: 0.9668 - val_loss: 0.1414\n",
      "Epoch 72/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9906 - loss: 0.0303 - val_accuracy: 0.9681 - val_loss: 0.1344\n",
      "Epoch 73/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9885 - loss: 0.0306 - val_accuracy: 0.9665 - val_loss: 0.1312\n",
      "Epoch 74/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9921 - loss: 0.0226 - val_accuracy: 0.9697 - val_loss: 0.1155\n",
      "Epoch 75/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9902 - loss: 0.0295 - val_accuracy: 0.9672 - val_loss: 0.1322\n",
      "Epoch 76/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9883 - loss: 0.0348 - val_accuracy: 0.9636 - val_loss: 0.1402\n",
      "Epoch 77/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9922 - loss: 0.0242 - val_accuracy: 0.9672 - val_loss: 0.1251\n",
      "Epoch 78/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9918 - loss: 0.0226 - val_accuracy: 0.9684 - val_loss: 0.1239\n",
      "Epoch 79/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9901 - loss: 0.0328 - val_accuracy: 0.9659 - val_loss: 0.1304\n",
      "Epoch 80/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9878 - loss: 0.0325 - val_accuracy: 0.9643 - val_loss: 0.1494\n",
      "Epoch 81/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9904 - loss: 0.0245 - val_accuracy: 0.9665 - val_loss: 0.1323\n",
      "Epoch 82/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9900 - loss: 0.0275 - val_accuracy: 0.9668 - val_loss: 0.1238\n",
      "Epoch 83/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9908 - loss: 0.0298 - val_accuracy: 0.9672 - val_loss: 0.1231\n",
      "Epoch 84/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9888 - loss: 0.0286 - val_accuracy: 0.9665 - val_loss: 0.1292\n",
      "Epoch 85/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9895 - loss: 0.0331 - val_accuracy: 0.9732 - val_loss: 0.1122\n",
      "Epoch 86/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9904 - loss: 0.0264 - val_accuracy: 0.9694 - val_loss: 0.1276\n",
      "Epoch 87/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9939 - loss: 0.0214 - val_accuracy: 0.9675 - val_loss: 0.1305\n",
      "Epoch 88/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9910 - loss: 0.0298 - val_accuracy: 0.9659 - val_loss: 0.1344\n",
      "Epoch 89/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9921 - loss: 0.0212 - val_accuracy: 0.9694 - val_loss: 0.1246\n",
      "Epoch 90/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9898 - loss: 0.0279 - val_accuracy: 0.9694 - val_loss: 0.1308\n",
      "Epoch 91/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9903 - loss: 0.0273 - val_accuracy: 0.9710 - val_loss: 0.1277\n",
      "Epoch 92/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9909 - loss: 0.0234 - val_accuracy: 0.9691 - val_loss: 0.1274\n",
      "Epoch 93/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9920 - loss: 0.0250 - val_accuracy: 0.9703 - val_loss: 0.1333\n",
      "Epoch 94/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9907 - loss: 0.0265 - val_accuracy: 0.9656 - val_loss: 0.1431\n",
      "Epoch 95/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9907 - loss: 0.0256 - val_accuracy: 0.9688 - val_loss: 0.1268\n",
      "Epoch 96/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9927 - loss: 0.0214 - val_accuracy: 0.9697 - val_loss: 0.1239\n",
      "Epoch 97/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9892 - loss: 0.0320 - val_accuracy: 0.9659 - val_loss: 0.1262\n",
      "Epoch 98/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9947 - loss: 0.0180 - val_accuracy: 0.9665 - val_loss: 0.1378\n",
      "Epoch 99/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9932 - loss: 0.0209 - val_accuracy: 0.9656 - val_loss: 0.1396\n",
      "Epoch 100/100\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9939 - loss: 0.0209 - val_accuracy: 0.9656 - val_loss: 0.1383\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 496us/step - accuracy: 0.9469 - loss: 0.3211\n",
      "Test loss: 0.4206145703792572\n",
      "Test accuracy: 0.925687849521637\n"
     ]
    }
   ],
   "source": [
    "def build_and_train_model(X_train, y_train, X_test, y_test, epochs=100, batch_size=50):\n",
    "    num_classes = len(np.unique(y_train))  # Determine the number of unique classes\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "\n",
    "    model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=1)\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "    return model, history, test_loss, test_accuracy\n",
    "\n",
    "# Example usage:\n",
    "model, history, test_loss, test_accuracy = build_and_train_model(X_train, y_train, X_test, y_test)\n",
    "print(\"Test loss:\", test_loss)\n",
    "print(\"Test accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-05 14:10:02,127] A new study created in memory with name: no-name-8fd5d20a-e31c-47c0-845a-92be38e5dc17\n",
      "[I 2024-05-05 14:10:19,755] Trial 0 finished with value: 0.9285163283348083 and parameters: {'learning_rate': 0.04361435912677133, 'dropout_rate': 0.6177066453749649, 'batch_size': 128}. Best is trial 0 with value: 0.9285163283348083.\n",
      "[I 2024-05-05 14:11:00,256] Trial 1 finished with value: 0.9408588409423828 and parameters: {'learning_rate': 3.610914682869556e-05, 'dropout_rate': 0.1584580341477136, 'batch_size': 32}. Best is trial 1 with value: 0.9408588409423828.\n",
      "[I 2024-05-05 14:11:41,350] Trial 2 finished with value: 0.9264592528343201 and parameters: {'learning_rate': 0.0041015267942916085, 'dropout_rate': 0.6053398055749268, 'batch_size': 32}. Best is trial 1 with value: 0.9408588409423828.\n",
      "[I 2024-05-05 14:11:59,581] Trial 3 finished with value: 0.9341732859611511 and parameters: {'learning_rate': 0.011213552508349664, 'dropout_rate': 0.2900131770771293, 'batch_size': 128}. Best is trial 1 with value: 0.9408588409423828.\n",
      "[I 2024-05-05 14:12:16,839] Trial 4 finished with value: 0.9395731687545776 and parameters: {'learning_rate': 0.021558340168687774, 'dropout_rate': 0.0708628900209235, 'batch_size': 128}. Best is trial 1 with value: 0.9408588409423828.\n",
      "[I 2024-05-05 14:12:29,630] Trial 5 finished with value: 0.8164052367210388 and parameters: {'learning_rate': 2.745733017996925e-05, 'dropout_rate': 0.65135920240469, 'batch_size': 256}. Best is trial 1 with value: 0.9408588409423828.\n",
      "[I 2024-05-05 14:12:42,316] Trial 6 finished with value: 0.9359732866287231 and parameters: {'learning_rate': 0.004329710414355209, 'dropout_rate': 0.03399592838853538, 'batch_size': 256}. Best is trial 1 with value: 0.9408588409423828.\n",
      "[I 2024-05-05 14:12:54,751] Trial 7 finished with value: 0.8827462196350098 and parameters: {'learning_rate': 6.125548410292495e-05, 'dropout_rate': 0.6527861717248961, 'batch_size': 256}. Best is trial 1 with value: 0.9408588409423828.\n",
      "[I 2024-05-05 14:13:36,037] Trial 8 finished with value: 0.9434301853179932 and parameters: {'learning_rate': 3.567310159258065e-05, 'dropout_rate': 0.16965729643188035, 'batch_size': 32}. Best is trial 8 with value: 0.9434301853179932.\n",
      "[I 2024-05-05 14:13:49,023] Trial 9 finished with value: 0.9357161521911621 and parameters: {'learning_rate': 0.00020105304265745358, 'dropout_rate': 0.5163661367832258, 'batch_size': 256}. Best is trial 8 with value: 0.9434301853179932.\n",
      "[I 2024-05-05 14:14:15,158] Trial 10 finished with value: 0.933401882648468 and parameters: {'learning_rate': 0.0003769484581785529, 'dropout_rate': 0.3336850712777969, 'batch_size': 64}. Best is trial 8 with value: 0.9434301853179932.\n",
      "[I 2024-05-05 14:14:55,529] Trial 11 finished with value: 0.9321162104606628 and parameters: {'learning_rate': 1.0800981477570774e-05, 'dropout_rate': 0.13231545631115713, 'batch_size': 32}. Best is trial 8 with value: 0.9434301853179932.\n",
      "[I 2024-05-05 14:15:35,669] Trial 12 finished with value: 0.9465157985687256 and parameters: {'learning_rate': 0.0001252388564104624, 'dropout_rate': 0.19796111166762118, 'batch_size': 32}. Best is trial 12 with value: 0.9465157985687256.\n",
      "[I 2024-05-05 14:16:15,300] Trial 13 finished with value: 0.9362304210662842 and parameters: {'learning_rate': 0.0002168325907688371, 'dropout_rate': 0.2356351822417334, 'batch_size': 32}. Best is trial 12 with value: 0.9465157985687256.\n",
      "[I 2024-05-05 14:16:56,422] Trial 14 finished with value: 0.9380303621292114 and parameters: {'learning_rate': 0.0007537215574157036, 'dropout_rate': 0.4321963069192526, 'batch_size': 32}. Best is trial 12 with value: 0.9465157985687256.\n",
      "[I 2024-05-05 14:17:22,921] Trial 15 finished with value: 0.9411159753799438 and parameters: {'learning_rate': 8.821470875035221e-05, 'dropout_rate': 0.2150305478712784, 'batch_size': 64}. Best is trial 12 with value: 0.9465157985687256.\n",
      "[I 2024-05-05 14:18:03,915] Trial 16 finished with value: 0.9318590760231018 and parameters: {'learning_rate': 0.0008615490180185775, 'dropout_rate': 0.3793646429471886, 'batch_size': 32}. Best is trial 12 with value: 0.9465157985687256.\n",
      "[I 2024-05-05 14:18:44,713] Trial 17 finished with value: 0.9290305972099304 and parameters: {'learning_rate': 1.1872095079223904e-05, 'dropout_rate': 0.11187512700680391, 'batch_size': 32}. Best is trial 12 with value: 0.9465157985687256.\n",
      "[I 2024-05-05 14:19:25,651] Trial 18 finished with value: 0.9465157985687256 and parameters: {'learning_rate': 0.00010433707588822679, 'dropout_rate': 0.23467822818422773, 'batch_size': 32}. Best is trial 12 with value: 0.9465157985687256.\n",
      "[I 2024-05-05 14:19:51,373] Trial 19 finished with value: 0.9393160343170166 and parameters: {'learning_rate': 0.002203203552831396, 'dropout_rate': 0.2658759708871924, 'batch_size': 64}. Best is trial 12 with value: 0.9465157985687256.\n",
      "[I 2024-05-05 14:20:33,207] Trial 20 finished with value: 0.9380303621292114 and parameters: {'learning_rate': 0.00013318704020217305, 'dropout_rate': 0.43698028965300323, 'batch_size': 32}. Best is trial 12 with value: 0.9465157985687256.\n",
      "[I 2024-05-05 14:21:14,714] Trial 21 finished with value: 0.9395731687545776 and parameters: {'learning_rate': 3.7091866964542624e-05, 'dropout_rate': 0.2012382092492783, 'batch_size': 32}. Best is trial 12 with value: 0.9465157985687256.\n",
      "[I 2024-05-05 14:21:55,642] Trial 22 finished with value: 0.9352018237113953 and parameters: {'learning_rate': 0.0004521581798164899, 'dropout_rate': 0.009298932164092666, 'batch_size': 32}. Best is trial 12 with value: 0.9465157985687256.\n",
      "[I 2024-05-05 14:22:36,201] Trial 23 finished with value: 0.9359732866287231 and parameters: {'learning_rate': 2.113612359603954e-05, 'dropout_rate': 0.1520280440693381, 'batch_size': 32}. Best is trial 12 with value: 0.9465157985687256.\n",
      "[I 2024-05-05 14:23:16,587] Trial 24 finished with value: 0.9449729919433594 and parameters: {'learning_rate': 8.191593393541493e-05, 'dropout_rate': 0.30442542416429536, 'batch_size': 32}. Best is trial 12 with value: 0.9465157985687256.\n",
      "[I 2024-05-05 14:23:57,345] Trial 25 finished with value: 0.9426587820053101 and parameters: {'learning_rate': 9.009361504694597e-05, 'dropout_rate': 0.3295284211234847, 'batch_size': 32}. Best is trial 12 with value: 0.9465157985687256.\n",
      "[I 2024-05-05 14:24:39,775] Trial 26 finished with value: 0.9375160932540894 and parameters: {'learning_rate': 0.00032912209251682216, 'dropout_rate': 0.38239278125655446, 'batch_size': 32}. Best is trial 12 with value: 0.9465157985687256.\n",
      "[I 2024-05-05 14:24:57,845] Trial 27 finished with value: 0.9390588998794556 and parameters: {'learning_rate': 0.0014383497617102114, 'dropout_rate': 0.27285009374530395, 'batch_size': 128}. Best is trial 12 with value: 0.9465157985687256.\n",
      "[I 2024-05-05 14:25:24,426] Trial 28 finished with value: 0.941887378692627 and parameters: {'learning_rate': 0.00014788653816482492, 'dropout_rate': 0.08090338629390637, 'batch_size': 64}. Best is trial 12 with value: 0.9465157985687256.\n",
      "[I 2024-05-05 14:25:42,168] Trial 29 finished with value: 0.9287734627723694 and parameters: {'learning_rate': 0.06551358035925464, 'dropout_rate': 0.502740872332779, 'batch_size': 128}. Best is trial 12 with value: 0.9465157985687256.\n",
      "[I 2024-05-05 14:26:23,433] Trial 30 finished with value: 0.9406017065048218 and parameters: {'learning_rate': 7.542003891533454e-05, 'dropout_rate': 0.2024331817738465, 'batch_size': 32}. Best is trial 12 with value: 0.9465157985687256.\n",
      "[I 2024-05-05 14:27:04,458] Trial 31 finished with value: 0.9439444541931152 and parameters: {'learning_rate': 5.736333363912037e-05, 'dropout_rate': 0.1675535902151804, 'batch_size': 32}. Best is trial 12 with value: 0.9465157985687256.\n",
      "[I 2024-05-05 14:27:47,431] Trial 32 finished with value: 0.9454872608184814 and parameters: {'learning_rate': 5.283276970105753e-05, 'dropout_rate': 0.3039393675949318, 'batch_size': 32}. Best is trial 12 with value: 0.9465157985687256.\n",
      "[I 2024-05-05 14:28:29,898] Trial 33 finished with value: 0.9272306561470032 and parameters: {'learning_rate': 1.8929516069063695e-05, 'dropout_rate': 0.30169079720742564, 'batch_size': 32}. Best is trial 12 with value: 0.9465157985687256.\n",
      "[I 2024-05-05 14:29:11,809] Trial 34 finished with value: 0.942401647567749 and parameters: {'learning_rate': 0.0001301157931836029, 'dropout_rate': 0.24168604375430722, 'batch_size': 32}. Best is trial 12 with value: 0.9465157985687256.\n",
      "[I 2024-05-05 14:29:53,173] Trial 35 finished with value: 0.9377732276916504 and parameters: {'learning_rate': 4.598016851199659e-05, 'dropout_rate': 0.3871056353422695, 'batch_size': 32}. Best is trial 12 with value: 0.9465157985687256.\n",
      "[I 2024-05-05 14:30:10,540] Trial 36 finished with value: 0.9431730508804321 and parameters: {'learning_rate': 0.00022024390816990577, 'dropout_rate': 0.29010477590447264, 'batch_size': 128}. Best is trial 12 with value: 0.9465157985687256.\n",
      "[I 2024-05-05 14:30:50,684] Trial 37 finished with value: 0.9413731098175049 and parameters: {'learning_rate': 0.0005127771787114808, 'dropout_rate': 0.3135724431514706, 'batch_size': 32}. Best is trial 12 with value: 0.9465157985687256.\n",
      "[I 2024-05-05 14:31:04,240] Trial 38 finished with value: 0.851632833480835 and parameters: {'learning_rate': 1.8672059562856248e-05, 'dropout_rate': 0.43549342662995494, 'batch_size': 256}. Best is trial 12 with value: 0.9465157985687256.\n",
      "[I 2024-05-05 14:31:45,084] Trial 39 finished with value: 0.9413731098175049 and parameters: {'learning_rate': 0.011242474800802402, 'dropout_rate': 0.358622554740589, 'batch_size': 32}. Best is trial 12 with value: 0.9465157985687256.\n",
      "[I 2024-05-05 14:32:26,087] Trial 40 finished with value: 0.9339161515235901 and parameters: {'learning_rate': 3.0830020350415276e-05, 'dropout_rate': 0.24147819763157619, 'batch_size': 32}. Best is trial 12 with value: 0.9465157985687256.\n",
      "[I 2024-05-05 14:33:06,639] Trial 41 finished with value: 0.9375160932540894 and parameters: {'learning_rate': 5.719361053470232e-05, 'dropout_rate': 0.17026238565743418, 'batch_size': 32}. Best is trial 12 with value: 0.9465157985687256.\n",
      "[I 2024-05-05 14:33:46,619] Trial 42 finished with value: 0.9470300674438477 and parameters: {'learning_rate': 9.046882745965463e-05, 'dropout_rate': 0.18391531398895708, 'batch_size': 32}. Best is trial 42 with value: 0.9470300674438477.\n",
      "[I 2024-05-05 14:33:59,211] Trial 43 finished with value: 0.9316019415855408 and parameters: {'learning_rate': 9.970122184242338e-05, 'dropout_rate': 0.10032612845796954, 'batch_size': 256}. Best is trial 42 with value: 0.9470300674438477.\n",
      "[I 2024-05-05 14:34:40,389] Trial 44 finished with value: 0.9395731687545776 and parameters: {'learning_rate': 0.00020091106613183196, 'dropout_rate': 0.05345031308410356, 'batch_size': 32}. Best is trial 42 with value: 0.9470300674438477.\n",
      "[I 2024-05-05 14:35:21,430] Trial 45 finished with value: 0.9390588998794556 and parameters: {'learning_rate': 0.000324186681628659, 'dropout_rate': 0.26647659264380474, 'batch_size': 32}. Best is trial 42 with value: 0.9470300674438477.\n",
      "[I 2024-05-05 14:35:39,171] Trial 46 finished with value: 0.9370018243789673 and parameters: {'learning_rate': 4.4541622412484866e-05, 'dropout_rate': 0.19703380791965333, 'batch_size': 128}. Best is trial 42 with value: 0.9470300674438477.\n",
      "[I 2024-05-05 14:36:04,861] Trial 47 finished with value: 0.9339161515235901 and parameters: {'learning_rate': 2.677588580884345e-05, 'dropout_rate': 0.3506491596667325, 'batch_size': 64}. Best is trial 42 with value: 0.9470300674438477.\n",
      "[I 2024-05-05 14:36:44,989] Trial 48 finished with value: 0.9395731687545776 and parameters: {'learning_rate': 0.00012556119409951568, 'dropout_rate': 0.2208243331604866, 'batch_size': 32}. Best is trial 42 with value: 0.9470300674438477.\n",
      "[I 2024-05-05 14:36:57,713] Trial 49 finished with value: 0.9380303621292114 and parameters: {'learning_rate': 6.686248748476142e-05, 'dropout_rate': 0.13562520979427922, 'batch_size': 256}. Best is trial 42 with value: 0.9470300674438477.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "  Value: 0.9470300674438477\n",
      "  Params: \n",
      "    learning_rate: 9.046882745965463e-05\n",
      "    dropout_rate: 0.18391531398895708\n",
      "    batch_size: 32\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    # Предлагаем параметры\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
    "    dropout_rate = trial.suggest_uniform('dropout_rate', 0.0, 0.7)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    epochs = 100  # Можно также оптимизировать количество эпох\n",
    "\n",
    "    # Создание модели\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(len(np.unique(y_train)), activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Компиляция модели\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Обучение модели\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=0)\n",
    "    \n",
    "    # Оценка модели\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    return accuracy  # Максимизация точности\n",
    "\n",
    "# Создание исследования\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)  # Можно изменить количество испытаний в зависимости от времени/ресурсов\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 819us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.97      2608\n",
      "           1       0.99      0.97      0.98       246\n",
      "           2       0.94      0.99      0.96       279\n",
      "           3       0.81      0.67      0.73       252\n",
      "           4       0.86      0.97      0.91       246\n",
      "           5       0.96      0.94      0.95       258\n",
      "\n",
      "    accuracy                           0.95      3889\n",
      "   macro avg       0.92      0.92      0.92      3889\n",
      "weighted avg       0.95      0.95      0.95      3889\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Загрузка лучшей модели\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(trial.params['dropout_rate']),\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(trial.params['dropout_rate']),\n",
    "    Dense(len(np.unique(y_train)), activation='softmax')\n",
    "])\n",
    "model.compile(optimizer=Adam(learning_rate=trial.params['learning_rate']),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Повторное обучение лучшей модели\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=trial.params['batch_size'], verbose=0)\n",
    "\n",
    "# Предсказания\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Отчёт о классификации\n",
    "report = classification_report(y_test, y_pred_classes)\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
