{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import optuna\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# выбор модели\n",
    "keras_optuna = True\n",
    "logistic_regression_optuna = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to read OMG data from a CSV file\n",
    "def read_omg_csv(path_palm_data: str, \n",
    "                 n_omg_channels: int, \n",
    "                 n_acc_channels: int = 0, \n",
    "                 n_gyr_channels: int = 0, \n",
    "                 n_mag_channels: int = 0, \n",
    "                 n_enc_channels: int = 0,\n",
    "                 button_ch: bool = True, \n",
    "                 sync_ch: bool = True, \n",
    "                 timestamp_ch: bool = True) -> pd.DataFrame:\n",
    "    \n",
    "    df_raw = pd.read_csv(path_palm_data, sep=' ', \n",
    "                         header=None, \n",
    "                         skipfooter=1, \n",
    "                         skiprows=1, \n",
    "                         engine='python')\n",
    "    columns = np.arange(n_omg_channels).astype('str').tolist()\n",
    "    \n",
    "    for label, label_count in zip(['ACC', 'GYR', 'MAG', 'ENC'], \n",
    "                                  [n_acc_channels, n_gyr_channels, n_mag_channels, n_enc_channels]):\n",
    "        columns = columns + ['{}{}'.format(label, i) for i in range(label_count)]\n",
    "        \n",
    "    if button_ch:\n",
    "        columns = columns + ['BUTTON']\n",
    "        \n",
    "    if sync_ch:\n",
    "        columns = columns + ['SYNC']\n",
    "        \n",
    "    if timestamp_ch:\n",
    "        columns = columns + ['ts']\n",
    "        \n",
    "    df_raw.columns = columns\n",
    "    \n",
    "    return df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15679, 106), (15679,), (3889, 106), (3889,))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_training_data(path_palm_data, path_protocol_data, path_meta_data, \n",
    "                          n_omg_channels=50, n_acc_channels=3, n_gyr_channels=3, \n",
    "                          n_mag_channels=0, n_enc_channels=6, \n",
    "                          standardize=True, normalize=True,\n",
    "                          DO_REPLACE_TO_MOVING_AVERAGE=True, \n",
    "                          DO_CALCULATE_DERIVATIVE=True,\n",
    "                          DO_SHIFT_GESTURE=True,\n",
    "                          selected_channels='ALL'):\n",
    "    \"\"\"\n",
    "    Подготовка данных для обучения и тестирования из файлов данных palm, protocol и meta.\n",
    "    \n",
    "    Аргументы:\n",
    "    path_palm_data (str): Путь к файлу данных palm.\n",
    "    path_protocol_data (str): Путь к файлу данных protocol.\n",
    "    path_meta_data (str): Путь к файлу данных meta.\n",
    "    n_omg_channels, n_acc_channels, и т.д. (int): Количество каналов сенсоров.\n",
    "    standardize (bool): Если True, стандартизирует признаки.\n",
    "    normalize (bool): Если True, нормализует признаки.\n",
    "    DO_REPLACE_TO_MOVING_AVERAGE (bool): Если True, применяет скользящее среднее к данным OMG.\n",
    "    DO_CALCULATE_DERIVATIVE (bool): Если True, вычисляет производные данных OMG.\n",
    "    DO_SHIFT_GESTURE (bool): Если True, смещает целевой признак на максимальный скачок в данных.\n",
    "    selected_channels (str): Выбор каналов данных ('OMG', 'ACC_GYR', 'ALL').\n",
    "    \n",
    "    Возвращает:\n",
    "    tuple: Кортеж, содержащий данные для обучения и тестирования.\n",
    "    \"\"\"\n",
    "    # Чтение данных OMG\n",
    "    omg_data = read_omg_csv(path_palm_data, n_omg_channels, n_acc_channels, n_gyr_channels, \n",
    "                            n_mag_channels, n_enc_channels)\n",
    "    \n",
    "    # Чтение данных протокола и кодирование жестов\n",
    "    gestures_protocol = pd.read_csv(path_protocol_data)\n",
    "    le = LabelEncoder()\n",
    "    gestures_protocol['gesture'] = le.fit_transform(\n",
    "        gestures_protocol[[\n",
    "            \"Thumb\", \"Index\", \"Middle\", \"Ring\", \"Pinky\",\n",
    "            'Thumb_stretch', 'Index_stretch', 'Middle_stretch', 'Ring_stretch', 'Pinky_stretch'\n",
    "        ]].apply(lambda row: str(tuple(row)), axis=1)\n",
    "    )\n",
    "    \n",
    "    # Чтение метаинформации\n",
    "    df_meta = pd.read_csv(path_meta_data)\n",
    "    palm_file = path_palm_data.split('/')[-1]\n",
    "    last_train_idx = df_meta[df_meta['montage'] == palm_file].to_dict(orient='records')[0]['last_train_idx']\n",
    "    \n",
    "    # Синхронизация меток жестов с данными OMG, используя канал SYNC\n",
    "    y_cmd = np.array([gestures_protocol['gesture'].loc[s] for s in omg_data['SYNC'].values])\n",
    "    \n",
    "    # Подготовка названий признаков для данных OMG\n",
    "    OMG_CH = [str(i) for i in range(n_omg_channels)]\n",
    "    ACC_CH = ['ACC0', 'ACC1', 'ACC2']\n",
    "    GYR_CH = ['GYR0', 'GYR1', 'GYR2']\n",
    "    ALL_CH = OMG_CH + ACC_CH + GYR_CH\n",
    "\n",
    "    # Выбор каналов в соответствии с параметром selected_channels\n",
    "    if selected_channels == 'OMG':\n",
    "        selected_features = OMG_CH\n",
    "    elif selected_channels == 'ACC_GYR':\n",
    "        selected_features = ACC_CH + GYR_CH\n",
    "    else:\n",
    "        selected_features = ALL_CH\n",
    "    \n",
    "    if DO_REPLACE_TO_MOVING_AVERAGE:\n",
    "        # Замена на скользящее среднее\n",
    "        for col in selected_features:\n",
    "            omg_data[col] = omg_data[col].rolling(window=5).mean().bfill()\n",
    "    \n",
    "    if DO_CALCULATE_DERIVATIVE:\n",
    "        # Вычисление производных данных\n",
    "        OMG_DERIV = [f'{col}_deriv' for col in OMG_CH]\n",
    "        for col in OMG_CH:\n",
    "            omg_data[f'{col}_next'] = omg_data[col].shift(-1).ffill()\n",
    "            omg_data[f'{col}_deriv'] = omg_data[f'{col}_next'] - omg_data[col]\n",
    "        selected_features += OMG_DERIV\n",
    "\n",
    "    if DO_SHIFT_GESTURE:\n",
    "        # Смещение целевого признака\n",
    "        id_max = 0\n",
    "        cur_gesture = 0\n",
    "        for i in range(y_cmd.shape[0]):\n",
    "            if i < id_max:  # Пропускаем все значения до id_max\n",
    "                continue\n",
    "            prev_gesture = cur_gesture  # предыдущий жест\n",
    "            cur_gesture = y_cmd[i]  # текущий жест\n",
    "            if cur_gesture != prev_gesture:  # Если сменился жест\n",
    "                id_max = omg_data[OMG_DERIV][i:i+35].abs().sum(axis=1).idxmax()  # Нахождение максимального скачка\n",
    "                y_cmd[i:id_max] = prev_gesture  # Замена всех значений до id_max на предыдущий жест\n",
    "    \n",
    "    # Разделение данных на обучающие и тестовые наборы\n",
    "    X_train = omg_data[selected_features].iloc[:last_train_idx].values\n",
    "    y_train = y_cmd[:last_train_idx]\n",
    "    X_test = omg_data[selected_features].iloc[last_train_idx:].values\n",
    "    y_test = y_cmd[last_train_idx:]\n",
    "    \n",
    "    # Стандартизация и нормализация\n",
    "    if standardize:\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        \n",
    "    if normalize:\n",
    "        scaler = MinMaxScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "    \n",
    "    return (X_train, y_train), (X_test, y_test)\n",
    "\n",
    "path_palm_data = 'data/2023-05-31_17-14-41.palm'\n",
    "path_protocol_data = 'data/2023-05-31_17-14-41.palm.protocol.csv'\n",
    "path_meta_data = 'data/meta_information.csv'\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = prepare_training_data(path_palm_data, path_protocol_data, path_meta_data, standardize=True, normalize=False)\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Counter({0: 10677, 5: 1052, 3: 1032, 4: 1020, 1: 952, 2: 946}),\n",
       " Counter({0: 2608, 2: 279, 5: 258, 3: 252, 1: 246, 4: 246}))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Посчитаем, как распределены классы\n",
    "class_counts_train = Counter(y_train)\n",
    "class_counts_test = Counter(y_test)\n",
    "\n",
    "class_counts_train, class_counts_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, классы не сбалансированы: класс 0 значительно превосходит по количеству остальные классы в обоих наборах данных. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-08 10:27:27,670] A new study created in memory with name: no-name-89c57789-12d9-4b2f-b2e0-f04461e8aaaf\n",
      "[I 2024-05-08 10:28:59,166] Trial 15 finished with value: 0.9485728740692139 and parameters: {'k_neighbors': 10, 'sampling_strategy': 'minority', 'learning_rate': 0.0005568173112544624, 'dropout_rate': 0.19979605768010528, 'batch_size': 256}. Best is trial 15 with value: 0.9485728740692139.\n",
      "[I 2024-05-08 10:29:52,975] Trial 2 finished with value: 0.9547441601753235 and parameters: {'k_neighbors': 2, 'sampling_strategy': 'minority', 'learning_rate': 0.06333489009732164, 'dropout_rate': 0.5921049173591335, 'batch_size': 128}. Best is trial 2 with value: 0.9547441601753235.\n",
      "[I 2024-05-08 10:29:53,770] Trial 13 finished with value: 0.9454872608184814 and parameters: {'k_neighbors': 6, 'sampling_strategy': 'minority', 'learning_rate': 0.0066654008196855965, 'dropout_rate': 0.48003358676714386, 'batch_size': 128}. Best is trial 2 with value: 0.9547441601753235.\n",
      "[I 2024-05-08 10:30:34,100] Trial 7 finished with value: 0.9501157402992249 and parameters: {'k_neighbors': 7, 'sampling_strategy': 'auto', 'learning_rate': 2.0436427336742387e-05, 'dropout_rate': 0.10289795112375678, 'batch_size': 256}. Best is trial 2 with value: 0.9547441601753235.\n",
      "[I 2024-05-08 10:30:34,904] Trial 10 finished with value: 0.949858546257019 and parameters: {'k_neighbors': 6, 'sampling_strategy': 'all', 'learning_rate': 0.011546666270262995, 'dropout_rate': 0.6874321729588255, 'batch_size': 256}. Best is trial 2 with value: 0.9547441601753235.\n",
      "[I 2024-05-08 10:30:35,580] Trial 12 finished with value: 0.9519156813621521 and parameters: {'k_neighbors': 4, 'sampling_strategy': 'all', 'learning_rate': 9.458545432851643e-05, 'dropout_rate': 0.22378422880744483, 'batch_size': 256}. Best is trial 2 with value: 0.9547441601753235.\n",
      "[I 2024-05-08 10:31:27,918] Trial 16 finished with value: 0.9519156813621521 and parameters: {'k_neighbors': 3, 'sampling_strategy': 'minority', 'learning_rate': 0.00010090811987245188, 'dropout_rate': 0.22574373028230144, 'batch_size': 128}. Best is trial 2 with value: 0.9547441601753235.\n",
      "[I 2024-05-08 10:31:58,845] Trial 3 finished with value: 0.9452301263809204 and parameters: {'k_neighbors': 7, 'sampling_strategy': 'minority', 'learning_rate': 0.022755302946127258, 'dropout_rate': 0.4081290079813048, 'batch_size': 64}. Best is trial 2 with value: 0.9547441601753235.\n",
      "[I 2024-05-08 10:32:30,632] Trial 18 finished with value: 0.949601411819458 and parameters: {'k_neighbors': 6, 'sampling_strategy': 'minority', 'learning_rate': 0.09998291399584143, 'dropout_rate': 0.5155223087564182, 'batch_size': 128}. Best is trial 2 with value: 0.9547441601753235.\n",
      "[I 2024-05-08 10:33:14,870] Trial 8 finished with value: 0.951144278049469 and parameters: {'k_neighbors': 3, 'sampling_strategy': 'all', 'learning_rate': 0.006461211115432067, 'dropout_rate': 0.6341143974146426, 'batch_size': 128}. Best is trial 2 with value: 0.9547441601753235.\n",
      "[I 2024-05-08 10:33:15,613] Trial 11 finished with value: 0.9467729330062866 and parameters: {'k_neighbors': 5, 'sampling_strategy': 'auto', 'learning_rate': 0.00025514002416109814, 'dropout_rate': 0.10431354899361099, 'batch_size': 128}. Best is trial 2 with value: 0.9547441601753235.\n",
      "[I 2024-05-08 10:33:16,353] Trial 5 finished with value: 0.9475443363189697 and parameters: {'k_neighbors': 9, 'sampling_strategy': 'not majority', 'learning_rate': 1.3774764804247918e-05, 'dropout_rate': 0.28039642575741475, 'batch_size': 128}. Best is trial 2 with value: 0.9547441601753235.\n",
      "[I 2024-05-08 10:34:01,322] Trial 19 finished with value: 0.9444587230682373 and parameters: {'k_neighbors': 3, 'sampling_strategy': 'auto', 'learning_rate': 0.00041698468637435624, 'dropout_rate': 0.13967085377238386, 'batch_size': 256}. Best is trial 2 with value: 0.9547441601753235.\n",
      "[I 2024-05-08 10:34:03,887] Trial 21 finished with value: 0.9501157402992249 and parameters: {'k_neighbors': 4, 'sampling_strategy': 'not majority', 'learning_rate': 0.0003325762490891042, 'dropout_rate': 0.6490429146283662, 'batch_size': 256}. Best is trial 2 with value: 0.9547441601753235.\n",
      "[I 2024-05-08 10:34:50,625] Trial 22 finished with value: 0.9478014707565308 and parameters: {'k_neighbors': 6, 'sampling_strategy': 'auto', 'learning_rate': 2.0344199986575864e-05, 'dropout_rate': 0.3723905652142699, 'batch_size': 256}. Best is trial 2 with value: 0.9547441601753235.\n",
      "[I 2024-05-08 10:35:07,608] Trial 24 finished with value: 0.9398303031921387 and parameters: {'k_neighbors': 8, 'sampling_strategy': 'minority', 'learning_rate': 0.02658339770903697, 'dropout_rate': 0.5879242190925914, 'batch_size': 128}. Best is trial 2 with value: 0.9547441601753235.\n",
      "[I 2024-05-08 10:35:17,325] Trial 23 finished with value: 0.9372589588165283 and parameters: {'k_neighbors': 8, 'sampling_strategy': 'auto', 'learning_rate': 0.007764874209779896, 'dropout_rate': 0.34376605414890693, 'batch_size': 256}. Best is trial 2 with value: 0.9547441601753235.\n",
      "[I 2024-05-08 10:36:01,150] Trial 14 finished with value: 0.9367446899414062 and parameters: {'k_neighbors': 6, 'sampling_strategy': 'minority', 'learning_rate': 0.023220909513010927, 'dropout_rate': 0.46195447365471515, 'batch_size': 32}. Best is trial 2 with value: 0.9547441601753235.\n",
      "[I 2024-05-08 10:36:22,538] Trial 20 finished with value: 0.9403445720672607 and parameters: {'k_neighbors': 10, 'sampling_strategy': 'all', 'learning_rate': 0.0035271134857048233, 'dropout_rate': 0.1267096463821265, 'batch_size': 128}. Best is trial 2 with value: 0.9547441601753235.\n",
      "[I 2024-05-08 10:37:59,848] Trial 9 finished with value: 0.9449729919433594 and parameters: {'k_neighbors': 6, 'sampling_strategy': 'auto', 'learning_rate': 1.0542657319178082e-05, 'dropout_rate': 0.03576212698539655, 'batch_size': 64}. Best is trial 2 with value: 0.9547441601753235.\n",
      "[I 2024-05-08 10:40:29,011] Trial 17 finished with value: 0.949344277381897 and parameters: {'k_neighbors': 8, 'sampling_strategy': 'auto', 'learning_rate': 0.009804938937457403, 'dropout_rate': 0.40001299476987534, 'batch_size': 64}. Best is trial 2 with value: 0.9547441601753235.\n",
      "[I 2024-05-08 10:48:02,383] Trial 34 finished with value: 0.941887378692627 and parameters: {'k_neighbors': 2, 'sampling_strategy': 'all', 'learning_rate': 0.0017524395231164516, 'dropout_rate': 0.2957316440174571, 'batch_size': 64}. Best is trial 2 with value: 0.9547441601753235.\n",
      "[I 2024-05-08 10:49:17,314] Trial 1 finished with value: 0.9488300085067749 and parameters: {'k_neighbors': 8, 'sampling_strategy': 'all', 'learning_rate': 0.01465475188207741, 'dropout_rate': 0.6652060192466505, 'batch_size': 32}. Best is trial 2 with value: 0.9547441601753235.\n",
      "[I 2024-05-08 10:49:20,751] Trial 6 finished with value: 0.9550012946128845 and parameters: {'k_neighbors': 4, 'sampling_strategy': 'auto', 'learning_rate': 1.204428298078834e-05, 'dropout_rate': 0.17362523752432465, 'batch_size': 32}. Best is trial 6 with value: 0.9550012946128845.\n",
      "[I 2024-05-08 10:49:21,844] Trial 0 finished with value: 0.9547441601753235 and parameters: {'k_neighbors': 6, 'sampling_strategy': 'all', 'learning_rate': 0.0448107530978819, 'dropout_rate': 0.5485352976256447, 'batch_size': 32}. Best is trial 6 with value: 0.9550012946128845.\n",
      "[I 2024-05-08 10:49:22,947] Trial 4 finished with value: 0.9506300091743469 and parameters: {'k_neighbors': 3, 'sampling_strategy': 'all', 'learning_rate': 0.011866803179273017, 'dropout_rate': 0.5161992896097942, 'batch_size': 32}. Best is trial 6 with value: 0.9550012946128845.\n",
      "[I 2024-05-08 10:52:21,415] Trial 38 finished with value: 0.9519156813621521 and parameters: {'k_neighbors': 2, 'sampling_strategy': 'minority', 'learning_rate': 9.106418994073982e-05, 'dropout_rate': 0.20095583026875358, 'batch_size': 128}. Best is trial 6 with value: 0.9550012946128845.\n",
      "[I 2024-05-08 10:56:11,652] Trial 26 finished with value: 0.950887143611908 and parameters: {'k_neighbors': 2, 'sampling_strategy': 'not majority', 'learning_rate': 1.4090927576920628e-05, 'dropout_rate': 0.29125346697105503, 'batch_size': 32}. Best is trial 6 with value: 0.9550012946128845.\n",
      "[I 2024-05-08 10:56:11,998] Trial 25 finished with value: 0.9457443952560425 and parameters: {'k_neighbors': 2, 'sampling_strategy': 'not majority', 'learning_rate': 0.0012962474847117726, 'dropout_rate': 0.5803754544186385, 'batch_size': 32}. Best is trial 6 with value: 0.9550012946128845.\n",
      "[I 2024-05-08 10:56:15,713] Trial 27 finished with value: 0.9501157402992249 and parameters: {'k_neighbors': 2, 'sampling_strategy': 'all', 'learning_rate': 0.0016387952746513537, 'dropout_rate': 0.400641983043167, 'batch_size': 32}. Best is trial 6 with value: 0.9550012946128845.\n",
      "[I 2024-05-08 10:57:06,558] Trial 28 finished with value: 0.949344277381897 and parameters: {'k_neighbors': 2, 'sampling_strategy': 'all', 'learning_rate': 6.570340060253527e-05, 'dropout_rate': 0.34068784889664017, 'batch_size': 32}. Best is trial 6 with value: 0.9550012946128845.\n",
      "[I 2024-05-08 10:57:08,940] Trial 29 finished with value: 0.9483157396316528 and parameters: {'k_neighbors': 2, 'sampling_strategy': 'all', 'learning_rate': 0.0022028203519300054, 'dropout_rate': 0.012186164354378748, 'batch_size': 32}. Best is trial 6 with value: 0.9550012946128845.\n",
      "[I 2024-05-08 10:58:12,310] Trial 30 finished with value: 0.9436873197555542 and parameters: {'k_neighbors': 2, 'sampling_strategy': 'all', 'learning_rate': 0.0018965188507727622, 'dropout_rate': 0.5519163148839044, 'batch_size': 32}. Best is trial 6 with value: 0.9550012946128845.\n",
      "[I 2024-05-08 10:58:20,613] Trial 37 finished with value: 0.9542298913002014 and parameters: {'k_neighbors': 2, 'sampling_strategy': 'minority', 'learning_rate': 8.95588279498901e-05, 'dropout_rate': 0.22657772580597332, 'batch_size': 32}. Best is trial 6 with value: 0.9550012946128845.\n",
      "[I 2024-05-08 10:58:27,786] Trial 31 finished with value: 0.941887378692627 and parameters: {'k_neighbors': 2, 'sampling_strategy': 'all', 'learning_rate': 0.0016884172917520318, 'dropout_rate': 0.2987313258990165, 'batch_size': 32}. Best is trial 6 with value: 0.9550012946128845.\n",
      "[I 2024-05-08 10:58:37,306] Trial 32 finished with value: 0.9501157402992249 and parameters: {'k_neighbors': 2, 'sampling_strategy': 'all', 'learning_rate': 0.0022376595098253568, 'dropout_rate': 0.03101106542303017, 'batch_size': 32}. Best is trial 6 with value: 0.9550012946128845.\n",
      "[I 2024-05-08 10:59:20,001] Trial 33 finished with value: 0.9480586051940918 and parameters: {'k_neighbors': 2, 'sampling_strategy': 'all', 'learning_rate': 0.0021144741356079336, 'dropout_rate': 0.010775049796654335, 'batch_size': 32}. Best is trial 6 with value: 0.9550012946128845.\n",
      "[I 2024-05-08 11:01:12,152] Trial 35 finished with value: 0.9534584879875183 and parameters: {'k_neighbors': 2, 'sampling_strategy': 'all', 'learning_rate': 7.492772073100374e-05, 'dropout_rate': 0.26687960666043137, 'batch_size': 32}. Best is trial 6 with value: 0.9550012946128845.\n",
      "[I 2024-05-08 11:03:01,886] Trial 36 finished with value: 0.9570583701133728 and parameters: {'k_neighbors': 2, 'sampling_strategy': 'all', 'learning_rate': 0.00010322646860398609, 'dropout_rate': 0.2609382632562011, 'batch_size': 32}. Best is trial 36 with value: 0.9570583701133728.\n",
      "[I 2024-05-08 11:04:42,850] Trial 49 finished with value: 0.9483157396316528 and parameters: {'k_neighbors': 4, 'sampling_strategy': 'minority', 'learning_rate': 0.05904615770703714, 'dropout_rate': 0.6234081557396758, 'batch_size': 32}. Best is trial 36 with value: 0.9570583701133728.\n",
      "[I 2024-05-08 11:07:13,345] Trial 41 finished with value: 0.9490871429443359 and parameters: {'k_neighbors': 2, 'sampling_strategy': 'not majority', 'learning_rate': 0.08669362603894616, 'dropout_rate': 0.5858781033434787, 'batch_size': 32}. Best is trial 36 with value: 0.9570583701133728.\n",
      "[I 2024-05-08 11:07:13,900] Trial 40 finished with value: 0.9470300674438477 and parameters: {'k_neighbors': 2, 'sampling_strategy': 'not majority', 'learning_rate': 0.06424745178881429, 'dropout_rate': 0.5474412159765886, 'batch_size': 32}. Best is trial 36 with value: 0.9570583701133728.\n",
      "[I 2024-05-08 11:07:14,058] Trial 39 finished with value: 0.9503728747367859 and parameters: {'k_neighbors': 2, 'sampling_strategy': 'all', 'learning_rate': 6.005323843857197e-05, 'dropout_rate': 0.026274814386644113, 'batch_size': 32}. Best is trial 36 with value: 0.9570583701133728.\n",
      "[I 2024-05-08 11:08:08,413] Trial 42 finished with value: 0.95140141248703 and parameters: {'k_neighbors': 4, 'sampling_strategy': 'not majority', 'learning_rate': 0.06843716269176961, 'dropout_rate': 0.5738405147939508, 'batch_size': 32}. Best is trial 36 with value: 0.9570583701133728.\n",
      "[I 2024-05-08 11:09:04,526] Trial 43 finished with value: 0.9516585469245911 and parameters: {'k_neighbors': 4, 'sampling_strategy': 'not majority', 'learning_rate': 0.0814912284765575, 'dropout_rate': 0.5789471223091769, 'batch_size': 32}. Best is trial 36 with value: 0.9570583701133728.\n",
      "[I 2024-05-08 11:09:05,248] Trial 44 finished with value: 0.9465157985687256 and parameters: {'k_neighbors': 4, 'sampling_strategy': 'all', 'learning_rate': 0.0922558638466947, 'dropout_rate': 0.5772724314951803, 'batch_size': 32}. Best is trial 36 with value: 0.9570583701133728.\n",
      "[I 2024-05-08 11:09:05,581] Trial 45 finished with value: 0.9503728747367859 and parameters: {'k_neighbors': 4, 'sampling_strategy': 'auto', 'learning_rate': 0.09699386415123057, 'dropout_rate': 0.579623648018077, 'batch_size': 32}. Best is trial 36 with value: 0.9570583701133728.\n",
      "[I 2024-05-08 11:09:11,257] Trial 46 finished with value: 0.9290305972099304 and parameters: {'k_neighbors': 4, 'sampling_strategy': 'all', 'learning_rate': 0.07390252447252771, 'dropout_rate': 0.04150417510148546, 'batch_size': 32}. Best is trial 36 with value: 0.9570583701133728.\n",
      "[I 2024-05-08 11:09:11,287] Trial 47 finished with value: 0.9398303031921387 and parameters: {'k_neighbors': 4, 'sampling_strategy': 'all', 'learning_rate': 0.09128005973518089, 'dropout_rate': 0.5743853390615016, 'batch_size': 32}. Best is trial 36 with value: 0.9570583701133728.\n",
      "[I 2024-05-08 11:09:17,537] Trial 48 finished with value: 0.9388017654418945 and parameters: {'k_neighbors': 4, 'sampling_strategy': 'all', 'learning_rate': 0.09414549126136333, 'dropout_rate': 0.16032747626102145, 'batch_size': 32}. Best is trial 36 with value: 0.9570583701133728.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры:\n",
      "FrozenTrial(number=36, state=1, values=[0.9570583701133728], datetime_start=datetime.datetime(2024, 5, 8, 10, 40, 29, 64403), datetime_complete=datetime.datetime(2024, 5, 8, 11, 3, 1, 886744), params={'k_neighbors': 2, 'sampling_strategy': 'all', 'learning_rate': 0.00010322646860398609, 'dropout_rate': 0.2609382632562011, 'batch_size': 32}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'k_neighbors': IntDistribution(high=10, log=False, low=2, step=1), 'sampling_strategy': CategoricalDistribution(choices=('auto', 'minority', 'not majority', 'all')), 'learning_rate': FloatDistribution(high=0.1, log=True, low=1e-05, step=None), 'dropout_rate': FloatDistribution(high=0.7, log=False, low=0.0, step=None), 'batch_size': CategoricalDistribution(choices=(32, 64, 128, 256))}, trial_id=36, value=None)\n"
     ]
    }
   ],
   "source": [
    "if keras_optuna:\n",
    "    def build_and_train_model(X_train, y_train, X_test, y_test, trial):\n",
    "        # Параметры для SMOTE\n",
    "        k_neighbors = trial.suggest_int('k_neighbors', 2, 10)\n",
    "        sampling_strategy = trial.suggest_categorical('sampling_strategy', ['auto', 'minority', 'not majority', 'all'])\n",
    "    \n",
    "        # Применяем SMOTE\n",
    "        smote = SMOTE(k_neighbors=k_neighbors, sampling_strategy=sampling_strategy, n_jobs=-1)\n",
    "        X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "        # Параметры модели\n",
    "        learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
    "        dropout_rate = trial.suggest_uniform('dropout_rate', 0.0, 0.7)\n",
    "        batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "        epochs = 100\n",
    "\n",
    "        num_classes = len(np.unique(y_resampled))\n",
    "    \n",
    "    # Создание модели\n",
    "        model = Sequential([\n",
    "            Dense(128, activation='relu', input_shape=(X_resampled.shape[1],)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(dropout_rate),\n",
    "            Dense(128, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout(dropout_rate),\n",
    "            Dense(num_classes, activation='softmax')\n",
    "        ])\n",
    "    \n",
    "        # Компиляция модели\n",
    "        model.compile(optimizer=Adam(learning_rate=learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "        # Обучение модели\n",
    "        history = model.fit(X_resampled, y_resampled, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=0)\n",
    "    \n",
    "        # Оценка модели\n",
    "        test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "        return test_accuracy\n",
    "\n",
    "    def objective(trial):\n",
    "        return build_and_train_model(X_train, y_train, X_test, y_test, trial)\n",
    "\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=50, n_jobs=-1)\n",
    "\n",
    "    print(\"Лучшие параметры:\")\n",
    "    print(study.best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры: {'k_neighbors': 2, 'sampling_strategy': 'all', 'learning_rate': 0.00010322646860398609, 'dropout_rate': 0.2609382632562011, 'batch_size': 32}\n",
      "Epoch 1/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 960us/step - accuracy: 0.4399 - loss: 1.6274 - val_accuracy: 0.9336 - val_loss: 0.2354\n",
      "Epoch 2/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 879us/step - accuracy: 0.8130 - loss: 0.5166 - val_accuracy: 0.9758 - val_loss: 0.0917\n",
      "Epoch 3/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 941us/step - accuracy: 0.8970 - loss: 0.2978 - val_accuracy: 0.9769 - val_loss: 0.0679\n",
      "Epoch 4/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 902us/step - accuracy: 0.9348 - loss: 0.1993 - val_accuracy: 0.9858 - val_loss: 0.0405\n",
      "Epoch 5/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9521 - loss: 0.1435 - val_accuracy: 0.9899 - val_loss: 0.0278\n",
      "Epoch 6/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9644 - loss: 0.1130 - val_accuracy: 0.9882 - val_loss: 0.0274\n",
      "Epoch 7/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9697 - loss: 0.0972 - val_accuracy: 0.9886 - val_loss: 0.0264\n",
      "Epoch 8/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9728 - loss: 0.0854 - val_accuracy: 0.9913 - val_loss: 0.0162\n",
      "Epoch 9/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9773 - loss: 0.0756 - val_accuracy: 0.9941 - val_loss: 0.0141\n",
      "Epoch 10/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9806 - loss: 0.0635 - val_accuracy: 0.9955 - val_loss: 0.0104\n",
      "Epoch 11/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9826 - loss: 0.0567 - val_accuracy: 0.9956 - val_loss: 0.0096\n",
      "Epoch 12/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9836 - loss: 0.0529 - val_accuracy: 0.9948 - val_loss: 0.0124\n",
      "Epoch 13/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9850 - loss: 0.0480 - val_accuracy: 0.9971 - val_loss: 0.0060\n",
      "Epoch 14/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9858 - loss: 0.0457 - val_accuracy: 0.9974 - val_loss: 0.0068\n",
      "Epoch 15/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9867 - loss: 0.0423 - val_accuracy: 0.9970 - val_loss: 0.0063\n",
      "Epoch 16/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9877 - loss: 0.0393 - val_accuracy: 0.9966 - val_loss: 0.0073\n",
      "Epoch 17/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9893 - loss: 0.0347 - val_accuracy: 0.9985 - val_loss: 0.0044\n",
      "Epoch 18/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9904 - loss: 0.0324 - val_accuracy: 0.9972 - val_loss: 0.0060\n",
      "Epoch 19/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9894 - loss: 0.0332 - val_accuracy: 0.9975 - val_loss: 0.0052\n",
      "Epoch 20/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9909 - loss: 0.0292 - val_accuracy: 0.9979 - val_loss: 0.0046\n",
      "Epoch 21/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9896 - loss: 0.0291 - val_accuracy: 0.9988 - val_loss: 0.0038\n",
      "Epoch 22/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9919 - loss: 0.0271 - val_accuracy: 0.9986 - val_loss: 0.0043\n",
      "Epoch 23/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9912 - loss: 0.0275 - val_accuracy: 1.0000 - val_loss: 0.0012\n",
      "Epoch 24/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9924 - loss: 0.0247 - val_accuracy: 0.9993 - val_loss: 0.0030\n",
      "Epoch 25/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9920 - loss: 0.0243 - val_accuracy: 0.9982 - val_loss: 0.0035\n",
      "Epoch 26/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9927 - loss: 0.0215 - val_accuracy: 0.9970 - val_loss: 0.0065\n",
      "Epoch 27/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9936 - loss: 0.0200 - val_accuracy: 0.9982 - val_loss: 0.0037\n",
      "Epoch 28/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9923 - loss: 0.0228 - val_accuracy: 0.9992 - val_loss: 0.0028\n",
      "Epoch 29/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9935 - loss: 0.0195 - val_accuracy: 1.0000 - val_loss: 0.0019\n",
      "Epoch 30/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9932 - loss: 0.0191 - val_accuracy: 0.9983 - val_loss: 0.0036\n",
      "Epoch 31/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9939 - loss: 0.0190 - val_accuracy: 0.9993 - val_loss: 0.0022\n",
      "Epoch 32/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9934 - loss: 0.0194 - val_accuracy: 1.0000 - val_loss: 0.0018\n",
      "Epoch 33/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9944 - loss: 0.0171 - val_accuracy: 0.9973 - val_loss: 0.0061\n",
      "Epoch 34/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9948 - loss: 0.0158 - val_accuracy: 0.9998 - val_loss: 0.0025\n",
      "Epoch 35/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9946 - loss: 0.0165 - val_accuracy: 0.9984 - val_loss: 0.0051\n",
      "Epoch 36/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9954 - loss: 0.0151 - val_accuracy: 0.9998 - val_loss: 0.0026\n",
      "Epoch 37/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9949 - loss: 0.0166 - val_accuracy: 1.0000 - val_loss: 7.8488e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9942 - loss: 0.0164 - val_accuracy: 0.9994 - val_loss: 0.0037\n",
      "Epoch 39/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9950 - loss: 0.0145 - val_accuracy: 1.0000 - val_loss: 0.0010\n",
      "Epoch 40/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9947 - loss: 0.0156 - val_accuracy: 1.0000 - val_loss: 4.3725e-04\n",
      "Epoch 41/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9956 - loss: 0.0132 - val_accuracy: 1.0000 - val_loss: 8.5444e-04\n",
      "Epoch 42/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9958 - loss: 0.0128 - val_accuracy: 0.9995 - val_loss: 0.0025\n",
      "Epoch 43/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9965 - loss: 0.0119 - val_accuracy: 0.9987 - val_loss: 0.0032\n",
      "Epoch 44/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9957 - loss: 0.0138 - val_accuracy: 0.9993 - val_loss: 0.0028\n",
      "Epoch 45/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9958 - loss: 0.0131 - val_accuracy: 1.0000 - val_loss: 0.0010\n",
      "Epoch 46/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9966 - loss: 0.0105 - val_accuracy: 1.0000 - val_loss: 0.0013\n",
      "Epoch 47/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 991us/step - accuracy: 0.9970 - loss: 0.0105 - val_accuracy: 1.0000 - val_loss: 0.0017\n",
      "Epoch 48/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 938us/step - accuracy: 0.9962 - loss: 0.0120 - val_accuracy: 1.0000 - val_loss: 4.7804e-04\n",
      "Epoch 49/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 983us/step - accuracy: 0.9964 - loss: 0.0113 - val_accuracy: 1.0000 - val_loss: 6.4970e-04\n",
      "Epoch 50/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9964 - loss: 0.0101 - val_accuracy: 1.0000 - val_loss: 3.5625e-04\n",
      "Epoch 51/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9965 - loss: 0.0106 - val_accuracy: 1.0000 - val_loss: 2.9611e-04\n",
      "Epoch 52/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9971 - loss: 0.0092 - val_accuracy: 1.0000 - val_loss: 5.4374e-04\n",
      "Epoch 53/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9959 - loss: 0.0120 - val_accuracy: 1.0000 - val_loss: 2.6633e-04\n",
      "Epoch 54/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9964 - loss: 0.0114 - val_accuracy: 1.0000 - val_loss: 7.0893e-04\n",
      "Epoch 55/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9972 - loss: 0.0091 - val_accuracy: 1.0000 - val_loss: 6.2870e-04\n",
      "Epoch 56/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9973 - loss: 0.0083 - val_accuracy: 1.0000 - val_loss: 1.8550e-04\n",
      "Epoch 57/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9973 - loss: 0.0083 - val_accuracy: 1.0000 - val_loss: 8.0871e-04\n",
      "Epoch 58/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9964 - loss: 0.0105 - val_accuracy: 1.0000 - val_loss: 4.2473e-04\n",
      "Epoch 59/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9966 - loss: 0.0097 - val_accuracy: 1.0000 - val_loss: 1.9351e-04\n",
      "Epoch 60/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9973 - loss: 0.0083 - val_accuracy: 1.0000 - val_loss: 9.3861e-04\n",
      "Epoch 61/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9967 - loss: 0.0099 - val_accuracy: 1.0000 - val_loss: 5.6984e-04\n",
      "Epoch 62/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9974 - loss: 0.0078 - val_accuracy: 1.0000 - val_loss: 4.3342e-04\n",
      "Epoch 63/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9966 - loss: 0.0095 - val_accuracy: 1.0000 - val_loss: 5.0418e-04\n",
      "Epoch 64/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9970 - loss: 0.0081 - val_accuracy: 1.0000 - val_loss: 1.9028e-04\n",
      "Epoch 65/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9973 - loss: 0.0085 - val_accuracy: 1.0000 - val_loss: 9.9069e-05\n",
      "Epoch 66/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9975 - loss: 0.0068 - val_accuracy: 1.0000 - val_loss: 5.5005e-04\n",
      "Epoch 67/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9975 - loss: 0.0076 - val_accuracy: 1.0000 - val_loss: 1.4812e-04\n",
      "Epoch 68/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9974 - loss: 0.0074 - val_accuracy: 1.0000 - val_loss: 3.9977e-04\n",
      "Epoch 69/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9975 - loss: 0.0078 - val_accuracy: 1.0000 - val_loss: 3.1478e-04\n",
      "Epoch 70/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9971 - loss: 0.0077 - val_accuracy: 1.0000 - val_loss: 1.4677e-04\n",
      "Epoch 71/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9976 - loss: 0.0073 - val_accuracy: 1.0000 - val_loss: 4.1020e-04\n",
      "Epoch 72/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 995us/step - accuracy: 0.9971 - loss: 0.0079 - val_accuracy: 1.0000 - val_loss: 1.8281e-04\n",
      "Epoch 73/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 955us/step - accuracy: 0.9974 - loss: 0.0078 - val_accuracy: 1.0000 - val_loss: 7.7251e-04\n",
      "Epoch 74/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 971us/step - accuracy: 0.9980 - loss: 0.0062 - val_accuracy: 1.0000 - val_loss: 2.8882e-04\n",
      "Epoch 75/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9976 - loss: 0.0074 - val_accuracy: 1.0000 - val_loss: 5.5213e-05\n",
      "Epoch 76/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9979 - loss: 0.0069 - val_accuracy: 1.0000 - val_loss: 1.3756e-04\n",
      "Epoch 77/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9981 - loss: 0.0062 - val_accuracy: 1.0000 - val_loss: 4.9529e-04\n",
      "Epoch 78/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9977 - loss: 0.0071 - val_accuracy: 1.0000 - val_loss: 1.0984e-04\n",
      "Epoch 79/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9980 - loss: 0.0060 - val_accuracy: 1.0000 - val_loss: 5.4512e-04\n",
      "Epoch 80/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9979 - loss: 0.0059 - val_accuracy: 1.0000 - val_loss: 6.7673e-04\n",
      "Epoch 81/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9979 - loss: 0.0066 - val_accuracy: 1.0000 - val_loss: 4.5868e-05\n",
      "Epoch 82/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9980 - loss: 0.0070 - val_accuracy: 1.0000 - val_loss: 7.4380e-04\n",
      "Epoch 83/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9982 - loss: 0.0050 - val_accuracy: 1.0000 - val_loss: 1.7892e-04\n",
      "Epoch 84/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9981 - loss: 0.0057 - val_accuracy: 1.0000 - val_loss: 2.1423e-04\n",
      "Epoch 85/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9982 - loss: 0.0051 - val_accuracy: 1.0000 - val_loss: 1.5707e-04\n",
      "Epoch 86/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9984 - loss: 0.0057 - val_accuracy: 1.0000 - val_loss: 1.5596e-04\n",
      "Epoch 87/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9980 - loss: 0.0060 - val_accuracy: 1.0000 - val_loss: 3.9703e-04\n",
      "Epoch 88/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9983 - loss: 0.0050 - val_accuracy: 1.0000 - val_loss: 4.0765e-04\n",
      "Epoch 89/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9981 - loss: 0.0055 - val_accuracy: 1.0000 - val_loss: 2.0658e-04\n",
      "Epoch 90/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9983 - loss: 0.0052 - val_accuracy: 1.0000 - val_loss: 5.5000e-04\n",
      "Epoch 91/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9982 - loss: 0.0049 - val_accuracy: 1.0000 - val_loss: 1.2242e-04\n",
      "Epoch 92/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9980 - loss: 0.0058 - val_accuracy: 1.0000 - val_loss: 2.5816e-04\n",
      "Epoch 93/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9986 - loss: 0.0044 - val_accuracy: 1.0000 - val_loss: 1.2001e-04\n",
      "Epoch 94/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9984 - loss: 0.0046 - val_accuracy: 1.0000 - val_loss: 1.5662e-04\n",
      "Epoch 95/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9983 - loss: 0.0051 - val_accuracy: 1.0000 - val_loss: 5.5556e-04\n",
      "Epoch 96/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9984 - loss: 0.0052 - val_accuracy: 1.0000 - val_loss: 6.0261e-05\n",
      "Epoch 97/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9986 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 1.4246e-04\n",
      "Epoch 98/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9982 - loss: 0.0053 - val_accuracy: 1.0000 - val_loss: 2.3819e-05\n",
      "Epoch 99/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9982 - loss: 0.0048 - val_accuracy: 0.9998 - val_loss: 7.0674e-04\n",
      "Epoch 100/100\n",
      "\u001b[1m1602/1602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9983 - loss: 0.0048 - val_accuracy: 1.0000 - val_loss: 1.3870e-04\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 868us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97      2608\n",
      "           1       0.98      0.98      0.98       246\n",
      "           2       0.93      0.99      0.96       279\n",
      "           3       0.86      0.79      0.82       252\n",
      "           4       0.88      0.97      0.92       246\n",
      "           5       0.95      0.95      0.95       258\n",
      "\n",
      "    accuracy                           0.96      3889\n",
      "   macro avg       0.93      0.94      0.93      3889\n",
      "weighted avg       0.96      0.96      0.96      3889\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if keras_optuna:\n",
    "    best_params = study.best_trial.params\n",
    "    print(\"Лучшие параметры:\", best_params)\n",
    "\n",
    "    smote = SMOTE(k_neighbors=best_params['k_neighbors'], sampling_strategy=best_params['sampling_strategy'])\n",
    "    X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    def build_final_model(X_train, y_train):\n",
    "        model = Sequential([\n",
    "            Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(best_params['dropout_rate']),\n",
    "            Dense(128, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout(best_params['dropout_rate']),\n",
    "            Dense(len(np.unique(y_train)), activation='softmax')\n",
    "        ])\n",
    "\n",
    "        model.compile(optimizer=Adam(learning_rate=best_params['learning_rate']),\n",
    "                    loss='sparse_categorical_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "        model.fit(X_train, y_train, epochs=100, batch_size=best_params['batch_size'], validation_split=0.2, verbose=1)\n",
    "        return model\n",
    "\n",
    "    final_model = build_final_model(X_train_smote, y_train_smote)\n",
    "\n",
    "    # Предсказания\n",
    "    y_pred = final_model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    # Отчёт о классификации\n",
    "    report = classification_report(y_test, y_pred_classes)\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if logistic_regression_optuna:\n",
    "    def objective(trial):\n",
    "    \n",
    "        # Параметры для SMOTE\n",
    "        smote_k_neighbors = trial.suggest_int('smote_k_neighbors', 2, 15)\n",
    "        smote_sampling_strategy = trial.suggest_categorical('smote_sampling_strategy', ['auto', 'minority', 'not majority', 'all'])\n",
    "\n",
    "        # Параметры для логистической регрессии\n",
    "        C = trial.suggest_loguniform('C', 1e-5, 10)\n",
    "        max_iter = trial.suggest_int('max_iter', 1000, 10000)\n",
    "        penalty = trial.suggest_categorical('penalty', ['l1', 'l2', 'elasticnet'])\n",
    "        \n",
    "        # Установка совместимого решателя в зависимости от выбранной регуляризации\n",
    "        if penalty == 'l1':\n",
    "            solver = 'liblinear' # liblinear поддерживает только l1 и l2\n",
    "        elif penalty == 'l2':\n",
    "            solver = trial.suggest_categorical('solver', ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'])\n",
    "        elif penalty == 'elasticnet':\n",
    "            solver = 'saga'  # saga - единственный, который поддерживает elasticnet\n",
    "\n",
    "        # Применение SMOTE\n",
    "        smote = SMOTE(k_neighbors=smote_k_neighbors, sampling_strategy=smote_sampling_strategy)\n",
    "        X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "        # Обучение модели логистической регрессии\n",
    "        model = LogisticRegression(C=C, max_iter=max_iter, penalty=penalty, solver=solver, l1_ratio=0.5 if penalty == 'elasticnet' else None)\n",
    "        model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "        # Оценка модели\n",
    "        score = f1_score(\n",
    "            y_test, \n",
    "            model.predict(X_test), \n",
    "            average = 'micro'\n",
    "        )\n",
    "        return score\n",
    "\n",
    "    # Создание исследования\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=100, n_jobs=-1)\n",
    "\n",
    "    print(\"Лучшие параметры:\", study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if logistic_regression_optuna:\n",
    "    # Извлечение лучших параметров\n",
    "    best_params = study.best_trial.params\n",
    "    print(\"Лучшие параметры:\", best_params)\n",
    "\n",
    "    # Применение SMOTE с лучшими параметрами\n",
    "    smote = SMOTE(k_neighbors=best_params['smote_k_neighbors'], sampling_strategy=best_params['smote_sampling_strategy'])\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Выбор решателя в зависимости от типа регуляризации\n",
    "    if best_params['penalty'] == 'elasticnet':\n",
    "        solver = 'saga'  # saga - единственный решатель, поддерживающий elasticnet\n",
    "    elif best_params['penalty'] == 'l1':\n",
    "        solver = 'liblinear'  # liblinear - оптимальный выбор для l1 регуляризации\n",
    "    elif best_params['penalty'] == 'l2':\n",
    "        solver = best_params['solver'] \n",
    "    else:  # 'none'\n",
    "        solver = 'lbfgs'  # lbfgs хорошо подходит для отсутствия регуляризации\n",
    "\n",
    "    # Построение и обучение модели логистической регрессии\n",
    "    model = LogisticRegression(\n",
    "        C=best_params['C'],\n",
    "        max_iter=best_params['max_iter'],\n",
    "        penalty=best_params['penalty'],\n",
    "        solver=solver,\n",
    "        l1_ratio=0.5 if best_params['penalty'] == 'elasticnet' else None,\n",
    "        multi_class='auto',\n",
    "        class_weight={0: 1, 1: 1, 2: 1, 3: 3, 4: 1, 5: 1}\n",
    "    )\n",
    "\n",
    "    model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "    #Делаем предсказание класса\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    print(f'Метрики на валидационной выборке \\n\\\n",
    "    {classification_report(y_test, y_pred)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
