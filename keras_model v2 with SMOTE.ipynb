{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import optuna\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to read OMG data from a CSV file\n",
    "def read_omg_csv(path_palm_data: str, \n",
    "                 n_omg_channels: int, \n",
    "                 n_acc_channels: int = 0, \n",
    "                 n_gyr_channels: int = 0, \n",
    "                 n_mag_channels: int = 0, \n",
    "                 n_enc_channels: int = 0,\n",
    "                 button_ch: bool = True, \n",
    "                 sync_ch: bool = True, \n",
    "                 timestamp_ch: bool = True) -> pd.DataFrame:\n",
    "    \n",
    "    df_raw = pd.read_csv(path_palm_data, sep=' ', \n",
    "                         header=None, \n",
    "                         skipfooter=1, \n",
    "                         skiprows=1, \n",
    "                         engine='python')\n",
    "    columns = np.arange(n_omg_channels).astype('str').tolist()\n",
    "    \n",
    "    for label, label_count in zip(['ACC', 'GYR', 'MAG', 'ENC'], \n",
    "                                  [n_acc_channels, n_gyr_channels, n_mag_channels, n_enc_channels]):\n",
    "        columns = columns + ['{}{}'.format(label, i) for i in range(label_count)]\n",
    "        \n",
    "    if button_ch:\n",
    "        columns = columns + ['BUTTON']\n",
    "        \n",
    "    if sync_ch:\n",
    "        columns = columns + ['SYNC']\n",
    "        \n",
    "    if timestamp_ch:\n",
    "        columns = columns + ['ts']\n",
    "        \n",
    "    df_raw.columns = columns\n",
    "    \n",
    "    return df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15679, 106), (15679,), (3889, 106), (3889,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_training_data(path_palm_data, path_protocol_data, path_meta_data, \n",
    "                          n_omg_channels=50, n_acc_channels=3, n_gyr_channels=3, \n",
    "                          n_mag_channels=0, n_enc_channels=6, \n",
    "                          standardize=True, normalize=True,\n",
    "                          DO_REPLACE_TO_MOVING_AVERAGE=True, \n",
    "                          DO_CALCULATE_DERIVATIVE=True,\n",
    "                          DO_SHIFT_GESTURE=True,\n",
    "                          selected_channels='ALL'):\n",
    "    \"\"\"\n",
    "    Подготовка данных для обучения и тестирования из файлов данных palm, protocol и meta.\n",
    "    \n",
    "    Аргументы:\n",
    "    path_palm_data (str): Путь к файлу данных palm.\n",
    "    path_protocol_data (str): Путь к файлу данных protocol.\n",
    "    path_meta_data (str): Путь к файлу данных meta.\n",
    "    n_omg_channels, n_acc_channels, и т.д. (int): Количество каналов сенсоров.\n",
    "    standardize (bool): Если True, стандартизирует признаки.\n",
    "    normalize (bool): Если True, нормализует признаки.\n",
    "    DO_REPLACE_TO_MOVING_AVERAGE (bool): Если True, применяет скользящее среднее к данным OMG.\n",
    "    DO_CALCULATE_DERIVATIVE (bool): Если True, вычисляет производные данных OMG.\n",
    "    DO_SHIFT_GESTURE (bool): Если True, смещает целевой признак на максимальный скачок в данных.\n",
    "    selected_channels (str): Выбор каналов данных ('OMG', 'ACC_GYR', 'ALL').\n",
    "    \n",
    "    Возвращает:\n",
    "    tuple: Кортеж, содержащий данные для обучения и тестирования.\n",
    "    \"\"\"\n",
    "    # Чтение данных OMG\n",
    "    omg_data = read_omg_csv(path_palm_data, n_omg_channels, n_acc_channels, n_gyr_channels, \n",
    "                            n_mag_channels, n_enc_channels)\n",
    "    \n",
    "    # Чтение данных протокола и кодирование жестов\n",
    "    gestures_protocol = pd.read_csv(path_protocol_data)\n",
    "    le = LabelEncoder()\n",
    "    gestures_protocol['gesture'] = le.fit_transform(\n",
    "        gestures_protocol[[\n",
    "            \"Thumb\", \"Index\", \"Middle\", \"Ring\", \"Pinky\",\n",
    "            'Thumb_stretch', 'Index_stretch', 'Middle_stretch', 'Ring_stretch', 'Pinky_stretch'\n",
    "        ]].apply(lambda row: str(tuple(row)), axis=1)\n",
    "    )\n",
    "    \n",
    "    # Чтение метаинформации\n",
    "    df_meta = pd.read_csv(path_meta_data)\n",
    "    palm_file = path_palm_data.split('/')[-1]\n",
    "    last_train_idx = df_meta[df_meta['montage'] == palm_file].to_dict(orient='records')[0]['last_train_idx']\n",
    "    \n",
    "    # Синхронизация меток жестов с данными OMG, используя канал SYNC\n",
    "    y_cmd = np.array([gestures_protocol['gesture'].loc[s] for s in omg_data['SYNC'].values])\n",
    "    \n",
    "    # Подготовка названий признаков для данных OMG\n",
    "    OMG_CH = [str(i) for i in range(n_omg_channels)]\n",
    "    ACC_CH = ['ACC0', 'ACC1', 'ACC2']\n",
    "    GYR_CH = ['GYR0', 'GYR1', 'GYR2']\n",
    "    ALL_CH = OMG_CH + ACC_CH + GYR_CH\n",
    "\n",
    "    # Выбор каналов в соответствии с параметром selected_channels\n",
    "    if selected_channels == 'OMG':\n",
    "        selected_features = OMG_CH\n",
    "    elif selected_channels == 'ACC_GYR':\n",
    "        selected_features = ACC_CH + GYR_CH\n",
    "    else:\n",
    "        selected_features = ALL_CH\n",
    "    \n",
    "    if DO_REPLACE_TO_MOVING_AVERAGE:\n",
    "        # Замена на скользящее среднее\n",
    "        for col in selected_features:\n",
    "            omg_data[col] = omg_data[col].rolling(window=5).mean().bfill()\n",
    "    \n",
    "    if DO_CALCULATE_DERIVATIVE:\n",
    "        # Вычисление производных данных\n",
    "        OMG_DERIV = [f'{col}_deriv' for col in OMG_CH]\n",
    "        for col in OMG_CH:\n",
    "            omg_data[f'{col}_next'] = omg_data[col].shift(-1).ffill()\n",
    "            omg_data[f'{col}_deriv'] = omg_data[f'{col}_next'] - omg_data[col]\n",
    "        selected_features += OMG_DERIV\n",
    "\n",
    "    if DO_SHIFT_GESTURE:\n",
    "        # Смещение целевого признака\n",
    "        id_max = 0\n",
    "        cur_gesture = 0\n",
    "        for i in range(y_cmd.shape[0]):\n",
    "            if i < id_max:  # Пропускаем все значения до id_max\n",
    "                continue\n",
    "            prev_gesture = cur_gesture  # предыдущий жест\n",
    "            cur_gesture = y_cmd[i]  # текущий жест\n",
    "            if cur_gesture != prev_gesture:  # Если сменился жест\n",
    "                id_max = omg_data[OMG_DERIV][i:i+35].abs().sum(axis=1).idxmax()  # Нахождение максимального скачка\n",
    "                y_cmd[i:id_max] = prev_gesture  # Замена всех значений до id_max на предыдущий жест\n",
    "    \n",
    "    # Разделение данных на обучающие и тестовые наборы\n",
    "    X_train = omg_data[selected_features].iloc[:last_train_idx].values\n",
    "    y_train = y_cmd[:last_train_idx]\n",
    "    X_test = omg_data[selected_features].iloc[last_train_idx:].values\n",
    "    y_test = y_cmd[last_train_idx:]\n",
    "    \n",
    "    # Стандартизация и нормализация\n",
    "    if standardize:\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        \n",
    "    if normalize:\n",
    "        scaler = MinMaxScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "    \n",
    "    return (X_train, y_train), (X_test, y_test)\n",
    "\n",
    "path_palm_data = 'data/2023-05-31_17-14-41.palm'\n",
    "path_protocol_data = 'data/2023-05-31_17-14-41.palm.protocol.csv'\n",
    "path_meta_data = 'data/meta_information.csv'\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = prepare_training_data(path_palm_data, path_protocol_data, path_meta_data, standardize=True, normalize=False)\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Counter({0: 10677, 5: 1052, 3: 1032, 4: 1020, 1: 952, 2: 946}),\n",
       " Counter({0: 2608, 2: 279, 5: 258, 3: 252, 1: 246, 4: 246}))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Посчитаем, как распределены классы\n",
    "class_counts_train = Counter(y_train)\n",
    "class_counts_test = Counter(y_test)\n",
    "\n",
    "class_counts_train, class_counts_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, классы не сбалансированы: класс 0 значительно превосходит по количеству остальные классы в обоих наборах данных. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 10677, 3: 10677, 5: 10677, 1: 10677, 4: 10677, 2: 10677})\n"
     ]
    }
   ],
   "source": [
    "# Инициализация экземпляра SMOTE\n",
    "smote = SMOTE(k_neighbors=4, sampling_strategy='auto', random_state=42)\n",
    "\n",
    "# Применение SMOTE к тренировочным данным\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Подсчёт количества примеров каждого класса после применения SMOTE\n",
    "class_counts_train_smote = Counter(y_train_smote)\n",
    "\n",
    "print(class_counts_train_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6472 - loss: 1.0109 - val_accuracy: 0.9855 - val_loss: 0.0450\n",
      "Epoch 2/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9387 - loss: 0.1796 - val_accuracy: 0.9920 - val_loss: 0.0216\n",
      "Epoch 3/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9695 - loss: 0.0981 - val_accuracy: 0.9881 - val_loss: 0.0293\n",
      "Epoch 4/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9788 - loss: 0.0719 - val_accuracy: 0.9958 - val_loss: 0.0100\n",
      "Epoch 5/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9807 - loss: 0.0647 - val_accuracy: 0.9952 - val_loss: 0.0112\n",
      "Epoch 6/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9840 - loss: 0.0539 - val_accuracy: 0.9991 - val_loss: 0.0038\n",
      "Epoch 7/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9856 - loss: 0.0468 - val_accuracy: 0.9874 - val_loss: 0.0318\n",
      "Epoch 8/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9861 - loss: 0.0441 - val_accuracy: 0.9980 - val_loss: 0.0061\n",
      "Epoch 9/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9886 - loss: 0.0375 - val_accuracy: 0.9967 - val_loss: 0.0076\n",
      "Epoch 10/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9891 - loss: 0.0365 - val_accuracy: 0.9983 - val_loss: 0.0051\n",
      "Epoch 11/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9897 - loss: 0.0343 - val_accuracy: 0.9957 - val_loss: 0.0094\n",
      "Epoch 12/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9887 - loss: 0.0345 - val_accuracy: 0.9970 - val_loss: 0.0075\n",
      "Epoch 13/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9905 - loss: 0.0306 - val_accuracy: 0.9878 - val_loss: 0.0226\n",
      "Epoch 14/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9901 - loss: 0.0294 - val_accuracy: 0.9943 - val_loss: 0.0136\n",
      "Epoch 15/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9906 - loss: 0.0295 - val_accuracy: 0.9998 - val_loss: 0.0021\n",
      "Epoch 16/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9912 - loss: 0.0267 - val_accuracy: 0.9995 - val_loss: 0.0028\n",
      "Epoch 17/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9906 - loss: 0.0279 - val_accuracy: 0.9991 - val_loss: 0.0034\n",
      "Epoch 18/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9908 - loss: 0.0281 - val_accuracy: 0.9999 - val_loss: 9.3886e-04\n",
      "Epoch 19/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9913 - loss: 0.0272 - val_accuracy: 0.9999 - val_loss: 9.2554e-04\n",
      "Epoch 20/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9922 - loss: 0.0233 - val_accuracy: 1.0000 - val_loss: 0.0010\n",
      "Epoch 21/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9920 - loss: 0.0242 - val_accuracy: 0.9892 - val_loss: 0.0210\n",
      "Epoch 22/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9921 - loss: 0.0262 - val_accuracy: 0.9982 - val_loss: 0.0056\n",
      "Epoch 23/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9923 - loss: 0.0242 - val_accuracy: 1.0000 - val_loss: 5.0313e-04\n",
      "Epoch 24/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9928 - loss: 0.0221 - val_accuracy: 0.9999 - val_loss: 0.0014\n",
      "Epoch 25/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9937 - loss: 0.0195 - val_accuracy: 0.9939 - val_loss: 0.0125\n",
      "Epoch 26/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9928 - loss: 0.0207 - val_accuracy: 0.9967 - val_loss: 0.0063\n",
      "Epoch 27/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9927 - loss: 0.0213 - val_accuracy: 0.9970 - val_loss: 0.0067\n",
      "Epoch 28/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9940 - loss: 0.0184 - val_accuracy: 0.9994 - val_loss: 0.0039\n",
      "Epoch 29/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9933 - loss: 0.0237 - val_accuracy: 0.9988 - val_loss: 0.0048\n",
      "Epoch 30/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9945 - loss: 0.0169 - val_accuracy: 0.9984 - val_loss: 0.0051\n",
      "Epoch 31/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9939 - loss: 0.0185 - val_accuracy: 0.9999 - val_loss: 0.0016\n",
      "Epoch 32/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9939 - loss: 0.0183 - val_accuracy: 1.0000 - val_loss: 7.3099e-04\n",
      "Epoch 33/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9941 - loss: 0.0187 - val_accuracy: 0.9973 - val_loss: 0.0072\n",
      "Epoch 34/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9936 - loss: 0.0179 - val_accuracy: 0.9987 - val_loss: 0.0022\n",
      "Epoch 35/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9949 - loss: 0.0165 - val_accuracy: 0.9998 - val_loss: 0.0034\n",
      "Epoch 36/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9950 - loss: 0.0163 - val_accuracy: 1.0000 - val_loss: 0.0019\n",
      "Epoch 37/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9936 - loss: 0.0193 - val_accuracy: 1.0000 - val_loss: 5.6863e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9948 - loss: 0.0161 - val_accuracy: 0.9953 - val_loss: 0.0117\n",
      "Epoch 39/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9949 - loss: 0.0160 - val_accuracy: 1.0000 - val_loss: 0.0011\n",
      "Epoch 40/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9952 - loss: 0.0161 - val_accuracy: 0.9971 - val_loss: 0.0068\n",
      "Epoch 41/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9942 - loss: 0.0168 - val_accuracy: 0.9994 - val_loss: 0.0030\n",
      "Epoch 42/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9940 - loss: 0.0182 - val_accuracy: 1.0000 - val_loss: 0.0012\n",
      "Epoch 43/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9955 - loss: 0.0136 - val_accuracy: 1.0000 - val_loss: 7.8280e-04\n",
      "Epoch 44/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9942 - loss: 0.0174 - val_accuracy: 0.9996 - val_loss: 0.0024\n",
      "Epoch 45/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9956 - loss: 0.0150 - val_accuracy: 0.9980 - val_loss: 0.0060\n",
      "Epoch 46/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9960 - loss: 0.0153 - val_accuracy: 1.0000 - val_loss: 0.0014\n",
      "Epoch 47/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9950 - loss: 0.0154 - val_accuracy: 0.9999 - val_loss: 0.0026\n",
      "Epoch 48/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9953 - loss: 0.0150 - val_accuracy: 1.0000 - val_loss: 3.6202e-04\n",
      "Epoch 49/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9947 - loss: 0.0158 - val_accuracy: 0.9994 - val_loss: 0.0020\n",
      "Epoch 50/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9954 - loss: 0.0148 - val_accuracy: 1.0000 - val_loss: 5.6118e-04\n",
      "Epoch 51/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9959 - loss: 0.0123 - val_accuracy: 1.0000 - val_loss: 0.0021\n",
      "Epoch 52/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9961 - loss: 0.0129 - val_accuracy: 0.9994 - val_loss: 0.0032\n",
      "Epoch 53/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9963 - loss: 0.0122 - val_accuracy: 1.0000 - val_loss: 0.0010\n",
      "Epoch 54/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9955 - loss: 0.0146 - val_accuracy: 1.0000 - val_loss: 8.1028e-04\n",
      "Epoch 55/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9956 - loss: 0.0140 - val_accuracy: 0.9994 - val_loss: 0.0023\n",
      "Epoch 56/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9967 - loss: 0.0120 - val_accuracy: 0.9999 - val_loss: 0.0011\n",
      "Epoch 57/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9955 - loss: 0.0142 - val_accuracy: 0.9908 - val_loss: 0.0183\n",
      "Epoch 58/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9958 - loss: 0.0123 - val_accuracy: 1.0000 - val_loss: 8.8093e-04\n",
      "Epoch 59/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9963 - loss: 0.0118 - val_accuracy: 0.9999 - val_loss: 0.0013\n",
      "Epoch 60/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9964 - loss: 0.0121 - val_accuracy: 1.0000 - val_loss: 3.3029e-04\n",
      "Epoch 61/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9957 - loss: 0.0141 - val_accuracy: 0.9985 - val_loss: 0.0034\n",
      "Epoch 62/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9947 - loss: 0.0157 - val_accuracy: 1.0000 - val_loss: 3.2310e-04\n",
      "Epoch 63/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9961 - loss: 0.0115 - val_accuracy: 1.0000 - val_loss: 3.1983e-04\n",
      "Epoch 64/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9956 - loss: 0.0135 - val_accuracy: 1.0000 - val_loss: 7.4404e-04\n",
      "Epoch 65/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9964 - loss: 0.0112 - val_accuracy: 1.0000 - val_loss: 6.0867e-04\n",
      "Epoch 66/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9958 - loss: 0.0121 - val_accuracy: 0.9998 - val_loss: 0.0019\n",
      "Epoch 67/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9963 - loss: 0.0124 - val_accuracy: 1.0000 - val_loss: 6.7814e-04\n",
      "Epoch 68/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9966 - loss: 0.0116 - val_accuracy: 0.9994 - val_loss: 0.0027\n",
      "Epoch 69/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9963 - loss: 0.0115 - val_accuracy: 0.9999 - val_loss: 0.0026\n",
      "Epoch 70/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9966 - loss: 0.0108 - val_accuracy: 1.0000 - val_loss: 0.0014\n",
      "Epoch 71/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9962 - loss: 0.0113 - val_accuracy: 1.0000 - val_loss: 8.3609e-04\n",
      "Epoch 72/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9966 - loss: 0.0114 - val_accuracy: 0.9992 - val_loss: 0.0032\n",
      "Epoch 73/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9963 - loss: 0.0111 - val_accuracy: 1.0000 - val_loss: 2.5991e-04\n",
      "Epoch 74/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9960 - loss: 0.0111 - val_accuracy: 1.0000 - val_loss: 8.9377e-04\n",
      "Epoch 75/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9956 - loss: 0.0142 - val_accuracy: 1.0000 - val_loss: 5.3061e-04\n",
      "Epoch 76/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9967 - loss: 0.0106 - val_accuracy: 1.0000 - val_loss: 5.8647e-04\n",
      "Epoch 77/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9970 - loss: 0.0105 - val_accuracy: 0.9998 - val_loss: 0.0017\n",
      "Epoch 78/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9968 - loss: 0.0101 - val_accuracy: 1.0000 - val_loss: 3.5772e-04\n",
      "Epoch 79/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9956 - loss: 0.0125 - val_accuracy: 0.9988 - val_loss: 0.0028\n",
      "Epoch 80/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9961 - loss: 0.0108 - val_accuracy: 1.0000 - val_loss: 0.0013\n",
      "Epoch 81/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9957 - loss: 0.0134 - val_accuracy: 0.9981 - val_loss: 0.0041\n",
      "Epoch 82/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9965 - loss: 0.0116 - val_accuracy: 1.0000 - val_loss: 8.7450e-04\n",
      "Epoch 83/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9968 - loss: 0.0095 - val_accuracy: 0.9988 - val_loss: 0.0022\n",
      "Epoch 84/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9965 - loss: 0.0107 - val_accuracy: 0.9999 - val_loss: 4.9058e-04\n",
      "Epoch 85/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9968 - loss: 0.0104 - val_accuracy: 1.0000 - val_loss: 6.1714e-04\n",
      "Epoch 86/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9974 - loss: 0.0087 - val_accuracy: 1.0000 - val_loss: 2.7466e-04\n",
      "Epoch 87/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9964 - loss: 0.0111 - val_accuracy: 0.9980 - val_loss: 0.0046\n",
      "Epoch 88/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9967 - loss: 0.0093 - val_accuracy: 1.0000 - val_loss: 4.7699e-04\n",
      "Epoch 89/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9975 - loss: 0.0085 - val_accuracy: 0.9934 - val_loss: 0.0128\n",
      "Epoch 90/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9975 - loss: 0.0087 - val_accuracy: 0.9999 - val_loss: 0.0017\n",
      "Epoch 91/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9971 - loss: 0.0100 - val_accuracy: 1.0000 - val_loss: 3.9079e-04\n",
      "Epoch 92/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9969 - loss: 0.0094 - val_accuracy: 1.0000 - val_loss: 5.5704e-04\n",
      "Epoch 93/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9971 - loss: 0.0089 - val_accuracy: 1.0000 - val_loss: 8.1160e-04\n",
      "Epoch 94/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9967 - loss: 0.0104 - val_accuracy: 1.0000 - val_loss: 3.1875e-04\n",
      "Epoch 95/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9971 - loss: 0.0092 - val_accuracy: 0.9991 - val_loss: 0.0018\n",
      "Epoch 96/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9968 - loss: 0.0099 - val_accuracy: 1.0000 - val_loss: 7.1820e-04\n",
      "Epoch 97/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9974 - loss: 0.0095 - val_accuracy: 1.0000 - val_loss: 4.0876e-04\n",
      "Epoch 98/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9966 - loss: 0.0095 - val_accuracy: 1.0000 - val_loss: 6.6861e-04\n",
      "Epoch 99/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9974 - loss: 0.0073 - val_accuracy: 1.0000 - val_loss: 4.0875e-04\n",
      "Epoch 100/100\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9970 - loss: 0.0087 - val_accuracy: 1.0000 - val_loss: 3.4744e-04\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 513us/step - accuracy: 0.9641 - loss: 0.2106\n",
      "Test loss: 0.34831663966178894\n",
      "Test accuracy: 0.9478014707565308\n"
     ]
    }
   ],
   "source": [
    "def build_and_train_model(X_train, y_train, X_test, y_test, epochs=100, batch_size=50):\n",
    "    num_classes = len(np.unique(y_train))  # Determine the number of unique classes\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "\n",
    "    model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=1)\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "    return model, history, test_loss, test_accuracy\n",
    "\n",
    "# Example usage:\n",
    "model, history, test_loss, test_accuracy = build_and_train_model(X_train_smote, y_train_smote, X_test, y_test)\n",
    "print(\"Test loss:\", test_loss)\n",
    "print(\"Test accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-07 20:32:28,515] A new study created in memory with name: no-name-cb9b09b8-03d5-4375-bf0d-4e5f2ef7d4cc\n",
      "[I 2024-05-07 20:38:45,789] Trial 14 finished with value: 0.9382874965667725 and parameters: {'learning_rate': 2.9958343372419382e-05, 'dropout_rate': 0.6636902867318807, 'batch_size': 256}. Best is trial 14 with value: 0.9382874965667725.\n",
      "[I 2024-05-07 20:38:48,194] Trial 22 finished with value: 0.9485728740692139 and parameters: {'learning_rate': 0.0019660620328689304, 'dropout_rate': 0.29631700860217425, 'batch_size': 256}. Best is trial 22 with value: 0.9485728740692139.\n",
      "[I 2024-05-07 20:38:48,735] Trial 21 finished with value: 0.95140141248703 and parameters: {'learning_rate': 0.00017833008390955146, 'dropout_rate': 0.572273503996012, 'batch_size': 256}. Best is trial 21 with value: 0.95140141248703.\n",
      "[I 2024-05-07 20:38:49,022] Trial 11 finished with value: 0.9429159164428711 and parameters: {'learning_rate': 0.0551246872605571, 'dropout_rate': 0.007072490964556832, 'batch_size': 256}. Best is trial 21 with value: 0.95140141248703.\n",
      "[I 2024-05-07 20:38:49,920] Trial 23 finished with value: 0.9426587820053101 and parameters: {'learning_rate': 0.001740991952501778, 'dropout_rate': 0.2802839591002199, 'batch_size': 256}. Best is trial 21 with value: 0.95140141248703.\n",
      "[I 2024-05-07 20:38:50,009] Trial 5 finished with value: 0.9408588409423828 and parameters: {'learning_rate': 0.09363426169288797, 'dropout_rate': 0.19400138169998654, 'batch_size': 256}. Best is trial 21 with value: 0.95140141248703.\n",
      "[I 2024-05-07 20:38:54,360] Trial 19 finished with value: 0.9452301263809204 and parameters: {'learning_rate': 0.00031080805807175046, 'dropout_rate': 0.3238457130506627, 'batch_size': 256}. Best is trial 21 with value: 0.95140141248703.\n",
      "[I 2024-05-07 20:43:42,941] Trial 16 finished with value: 0.9465157985687256 and parameters: {'learning_rate': 0.0016053331661194873, 'dropout_rate': 0.48196556975132576, 'batch_size': 128}. Best is trial 21 with value: 0.95140141248703.\n",
      "[I 2024-05-07 20:43:47,019] Trial 6 finished with value: 0.9460015296936035 and parameters: {'learning_rate': 0.028030576978799654, 'dropout_rate': 0.0483301008912444, 'batch_size': 128}. Best is trial 21 with value: 0.95140141248703.\n",
      "[I 2024-05-07 20:43:49,713] Trial 29 finished with value: 0.9460015296936035 and parameters: {'learning_rate': 1.413040505985868e-05, 'dropout_rate': 0.15309619141918865, 'batch_size': 128}. Best is trial 21 with value: 0.95140141248703.\n",
      "[I 2024-05-07 20:43:50,431] Trial 28 finished with value: 0.9375160932540894 and parameters: {'learning_rate': 0.00047453987793322114, 'dropout_rate': 0.30548930702150806, 'batch_size': 128}. Best is trial 21 with value: 0.95140141248703.\n",
      "[I 2024-05-07 20:43:53,387] Trial 26 finished with value: 0.9503728747367859 and parameters: {'learning_rate': 0.0001025965828447901, 'dropout_rate': 0.5073783892269024, 'batch_size': 128}. Best is trial 21 with value: 0.95140141248703.\n",
      "[I 2024-05-07 20:44:20,775] Trial 10 finished with value: 0.9478014707565308 and parameters: {'learning_rate': 0.0003984680352460707, 'dropout_rate': 0.4415148522177658, 'batch_size': 128}. Best is trial 21 with value: 0.95140141248703.\n",
      "[I 2024-05-07 20:44:59,954] Trial 33 finished with value: 0.9370018243789673 and parameters: {'learning_rate': 0.03363302151596677, 'dropout_rate': 0.6320608049683893, 'batch_size': 256}. Best is trial 21 with value: 0.95140141248703.\n",
      "[I 2024-05-07 20:45:05,376] Trial 37 finished with value: 0.9346875548362732 and parameters: {'learning_rate': 2.9212855653763207e-05, 'dropout_rate': 0.6548119300403195, 'batch_size': 256}. Best is trial 21 with value: 0.95140141248703.\n",
      "[I 2024-05-07 20:45:07,119] Trial 35 finished with value: 0.949344277381897 and parameters: {'learning_rate': 0.013619548813763226, 'dropout_rate': 0.5351717577272342, 'batch_size': 256}. Best is trial 21 with value: 0.95140141248703.\n",
      "[I 2024-05-07 20:53:42,981] Trial 0 finished with value: 0.9452301263809204 and parameters: {'learning_rate': 0.0002458094742250235, 'dropout_rate': 0.4912583057965986, 'batch_size': 64}. Best is trial 21 with value: 0.95140141248703.\n",
      "[I 2024-05-07 20:53:46,552] Trial 24 finished with value: 0.9547441601753235 and parameters: {'learning_rate': 3.754621789416905e-05, 'dropout_rate': 0.40807076459005276, 'batch_size': 64}. Best is trial 24 with value: 0.9547441601753235.\n",
      "[I 2024-05-07 20:53:52,102] Trial 31 finished with value: 0.95140141248703 and parameters: {'learning_rate': 0.0010141727195562382, 'dropout_rate': 0.6369757172389976, 'batch_size': 64}. Best is trial 24 with value: 0.9547441601753235.\n",
      "[I 2024-05-07 20:53:52,171] Trial 12 finished with value: 0.9429159164428711 and parameters: {'learning_rate': 0.018257091294097052, 'dropout_rate': 0.14086955499081677, 'batch_size': 64}. Best is trial 24 with value: 0.9547441601753235.\n",
      "[I 2024-05-07 20:53:53,607] Trial 3 finished with value: 0.9475443363189697 and parameters: {'learning_rate': 1.609170046250565e-05, 'dropout_rate': 0.40182545910208817, 'batch_size': 64}. Best is trial 24 with value: 0.9547441601753235.\n",
      "[I 2024-05-07 20:53:53,698] Trial 15 finished with value: 0.9480586051940918 and parameters: {'learning_rate': 0.02517629013310989, 'dropout_rate': 0.2687327322160607, 'batch_size': 64}. Best is trial 24 with value: 0.9547441601753235.\n",
      "[I 2024-05-07 20:53:54,984] Trial 8 finished with value: 0.9393160343170166 and parameters: {'learning_rate': 0.004694915938702618, 'dropout_rate': 0.24007484243262617, 'batch_size': 64}. Best is trial 24 with value: 0.9547441601753235.\n",
      "[I 2024-05-07 20:53:59,073] Trial 17 finished with value: 0.9506300091743469 and parameters: {'learning_rate': 9.88414566112906e-05, 'dropout_rate': 0.3751299255347093, 'batch_size': 64}. Best is trial 24 with value: 0.9547441601753235.\n",
      "[I 2024-05-07 20:53:59,629] Trial 4 finished with value: 0.9460015296936035 and parameters: {'learning_rate': 0.000124656604810837, 'dropout_rate': 0.08090993600025362, 'batch_size': 64}. Best is trial 24 with value: 0.9547441601753235.\n",
      "[I 2024-05-07 20:54:00,133] Trial 1 finished with value: 0.949601411819458 and parameters: {'learning_rate': 0.00011498424080794458, 'dropout_rate': 0.35451758652036824, 'batch_size': 64}. Best is trial 24 with value: 0.9547441601753235.\n",
      "[I 2024-05-07 20:54:13,328] Trial 25 finished with value: 0.9521728157997131 and parameters: {'learning_rate': 8.460746826271022e-05, 'dropout_rate': 0.16996446443015473, 'batch_size': 64}. Best is trial 24 with value: 0.9547441601753235.\n",
      "[I 2024-05-07 20:54:25,776] Trial 39 finished with value: 0.9460015296936035 and parameters: {'learning_rate': 3.614217145680912e-05, 'dropout_rate': 0.6622834309133598, 'batch_size': 128}. Best is trial 24 with value: 0.9547441601753235.\n",
      "[I 2024-05-07 20:54:31,912] Trial 40 finished with value: 0.9413731098175049 and parameters: {'learning_rate': 0.01454152224862892, 'dropout_rate': 0.5955324055904283, 'batch_size': 128}. Best is trial 24 with value: 0.9547441601753235.\n",
      "[I 2024-05-07 20:58:09,977] Trial 38 finished with value: 0.9519156813621521 and parameters: {'learning_rate': 0.00012817481212778554, 'dropout_rate': 0.5932476618512408, 'batch_size': 64}. Best is trial 24 with value: 0.9547441601753235.\n",
      "[I 2024-05-07 21:01:08,500] Trial 41 finished with value: 0.9501157402992249 and parameters: {'learning_rate': 0.00010082119982546859, 'dropout_rate': 0.5245341391204293, 'batch_size': 64}. Best is trial 24 with value: 0.9547441601753235.\n",
      "[I 2024-05-07 21:01:55,600] Trial 46 finished with value: 0.951144278049469 and parameters: {'learning_rate': 0.0001168486288255784, 'dropout_rate': 0.5247913147931356, 'batch_size': 64}. Best is trial 24 with value: 0.9547441601753235.\n",
      "[I 2024-05-07 21:01:56,024] Trial 45 finished with value: 0.9516585469245911 and parameters: {'learning_rate': 7.691609873128456e-05, 'dropout_rate': 0.5241870776796536, 'batch_size': 64}. Best is trial 24 with value: 0.9547441601753235.\n",
      "[I 2024-05-07 21:05:12,384] Trial 9 finished with value: 0.950887143611908 and parameters: {'learning_rate': 2.9886192874400565e-05, 'dropout_rate': 0.3917804383079933, 'batch_size': 32}. Best is trial 24 with value: 0.9547441601753235.\n",
      "[I 2024-05-07 21:05:16,357] Trial 2 finished with value: 0.9516585469245911 and parameters: {'learning_rate': 1.5975821544538682e-05, 'dropout_rate': 0.607272038258131, 'batch_size': 32}. Best is trial 24 with value: 0.9547441601753235.\n",
      "[I 2024-05-07 21:05:19,142] Trial 7 finished with value: 0.9488300085067749 and parameters: {'learning_rate': 0.0022337022001244653, 'dropout_rate': 0.3993085979647599, 'batch_size': 32}. Best is trial 24 with value: 0.9547441601753235.\n",
      "[I 2024-05-07 21:05:21,825] Trial 30 finished with value: 0.9542298913002014 and parameters: {'learning_rate': 1.695080474940153e-05, 'dropout_rate': 0.27921770661170203, 'batch_size': 32}. Best is trial 24 with value: 0.9547441601753235.\n",
      "[I 2024-05-07 21:05:22,864] Trial 13 finished with value: 0.9449729919433594 and parameters: {'learning_rate': 0.007576124295580523, 'dropout_rate': 0.18939810271620242, 'batch_size': 32}. Best is trial 24 with value: 0.9547441601753235.\n",
      "[I 2024-05-07 21:05:24,425] Trial 20 finished with value: 0.9470300674438477 and parameters: {'learning_rate': 0.00749980278411798, 'dropout_rate': 0.12585392360913544, 'batch_size': 32}. Best is trial 24 with value: 0.9547441601753235.\n",
      "[I 2024-05-07 21:05:24,667] Trial 18 finished with value: 0.9475443363189697 and parameters: {'learning_rate': 0.06094066757011783, 'dropout_rate': 0.11048449802371449, 'batch_size': 32}. Best is trial 24 with value: 0.9547441601753235.\n",
      "[I 2024-05-07 21:05:25,014] Trial 27 finished with value: 0.950887143611908 and parameters: {'learning_rate': 0.00013915840470219508, 'dropout_rate': 0.31317352914186386, 'batch_size': 32}. Best is trial 24 with value: 0.9547441601753235.\n",
      "[I 2024-05-07 21:06:06,788] Trial 49 finished with value: 0.9462586641311646 and parameters: {'learning_rate': 7.469183798481922e-05, 'dropout_rate': 0.4185790610821577, 'batch_size': 64}. Best is trial 24 with value: 0.9547441601753235.\n",
      "[I 2024-05-07 21:06:54,218] Trial 32 finished with value: 0.9485728740692139 and parameters: {'learning_rate': 1.2213261974670603e-05, 'dropout_rate': 0.5012720482447642, 'batch_size': 32}. Best is trial 24 with value: 0.9547441601753235.\n",
      "[I 2024-05-07 21:06:54,868] Trial 34 finished with value: 0.9539727568626404 and parameters: {'learning_rate': 0.04070401789266031, 'dropout_rate': 0.579223631677143, 'batch_size': 32}. Best is trial 24 with value: 0.9547441601753235.\n",
      "[I 2024-05-07 21:06:59,971] Trial 36 finished with value: 0.9519156813621521 and parameters: {'learning_rate': 0.0005227924455739611, 'dropout_rate': 0.5958896746976463, 'batch_size': 32}. Best is trial 24 with value: 0.9547441601753235.\n",
      "[I 2024-05-07 21:07:40,076] Trial 42 finished with value: 0.9472872018814087 and parameters: {'learning_rate': 0.00010346017924486196, 'dropout_rate': 0.46909273249740774, 'batch_size': 32}. Best is trial 24 with value: 0.9547441601753235.\n",
      "[I 2024-05-07 21:07:41,486] Trial 43 finished with value: 0.9490871429443359 and parameters: {'learning_rate': 6.589024597019192e-05, 'dropout_rate': 0.5244104491639364, 'batch_size': 32}. Best is trial 24 with value: 0.9547441601753235.\n",
      "[I 2024-05-07 21:07:45,995] Trial 44 finished with value: 0.9485728740692139 and parameters: {'learning_rate': 8.20359832710504e-05, 'dropout_rate': 0.6256386006847835, 'batch_size': 32}. Best is trial 24 with value: 0.9547441601753235.\n",
      "[I 2024-05-07 21:07:47,460] Trial 47 finished with value: 0.9457443952560425 and parameters: {'learning_rate': 0.00014610786429670672, 'dropout_rate': 0.5661960435273266, 'batch_size': 32}. Best is trial 24 with value: 0.9547441601753235.\n",
      "[I 2024-05-07 21:08:23,907] Trial 48 finished with value: 0.9490871429443359 and parameters: {'learning_rate': 0.00012878738696534955, 'dropout_rate': 0.5724971672493251, 'batch_size': 32}. Best is trial 24 with value: 0.9547441601753235.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "  Value: 0.9547441601753235\n",
      "  Params: \n",
      "    learning_rate: 3.754621789416905e-05\n",
      "    dropout_rate: 0.40807076459005276\n",
      "    batch_size: 64\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    # Предлагаем параметры\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
    "    dropout_rate = trial.suggest_uniform('dropout_rate', 0.0, 0.7)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    epochs = 100  # Можно также оптимизировать количество эпох\n",
    "\n",
    "    # Создание модели\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(len(np.unique(y_train)), activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Компиляция модели\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Обучение модели\n",
    "    model.fit(X_train_smote, y_train_smote, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=0)\n",
    "    \n",
    "    # Оценка модели\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    return accuracy  # Максимизация точности\n",
    "\n",
    "# Создание исследования\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50, n_jobs=-1)  # Можно изменить количество испытаний в зависимости от времени/ресурсов\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 869us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.95      0.96      2608\n",
      "           1       0.98      0.98      0.98       246\n",
      "           2       0.93      0.99      0.95       279\n",
      "           3       0.77      0.79      0.78       252\n",
      "           4       0.87      0.97      0.92       246\n",
      "           5       0.94      0.95      0.94       258\n",
      "\n",
      "    accuracy                           0.95      3889\n",
      "   macro avg       0.91      0.94      0.92      3889\n",
      "weighted avg       0.95      0.95      0.95      3889\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Загрузка лучшей модели\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(trial.params['dropout_rate']),\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(trial.params['dropout_rate']),\n",
    "    Dense(len(np.unique(y_train)), activation='softmax')\n",
    "])\n",
    "model.compile(optimizer=Adam(learning_rate=trial.params['learning_rate']),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Повторное обучение лучшей модели\n",
    "model.fit(X_train_smote, y_train_smote, epochs=100, batch_size=trial.params['batch_size'], verbose=0)\n",
    "\n",
    "# Предсказания\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Отчёт о классификации\n",
    "report = classification_report(y_test, y_pred_classes)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-07 21:18:02,341] A new study created in memory with name: no-name-d68c4678-9c26-460b-87fe-8ed9a5a793d5\n",
      "[I 2024-05-07 21:18:47,622] Trial 0 finished with value: 0.9393160343170166 and parameters: {'k_neighbors': 7, 'sampling_strategy': 'minority', 'learning_rate': 0.00228183750225705, 'dropout_rate': 0.2757413619530013, 'batch_size': 64}. Best is trial 0 with value: 0.9393160343170166.\n",
      "[I 2024-05-07 21:20:35,064] Trial 1 finished with value: 0.9398303031921387 and parameters: {'k_neighbors': 10, 'sampling_strategy': 'all', 'learning_rate': 1.164216277565633e-05, 'dropout_rate': 0.6605432548210465, 'batch_size': 64}. Best is trial 1 with value: 0.9398303031921387.\n",
      "[I 2024-05-07 21:22:22,644] Trial 2 finished with value: 0.9483157396316528 and parameters: {'k_neighbors': 9, 'sampling_strategy': 'not majority', 'learning_rate': 0.016969931864945123, 'dropout_rate': 0.5189124851750998, 'batch_size': 64}. Best is trial 2 with value: 0.9483157396316528.\n",
      "[I 2024-05-07 21:24:59,638] Trial 3 finished with value: 0.9480586051940918 and parameters: {'k_neighbors': 9, 'sampling_strategy': 'auto', 'learning_rate': 0.09968188254455505, 'dropout_rate': 0.5757471570866306, 'batch_size': 32}. Best is trial 2 with value: 0.9483157396316528.\n",
      "[I 2024-05-07 21:26:03,658] Trial 4 finished with value: 0.942144513130188 and parameters: {'k_neighbors': 9, 'sampling_strategy': 'all', 'learning_rate': 0.0005984280333573213, 'dropout_rate': 0.5826797427362191, 'batch_size': 128}. Best is trial 2 with value: 0.9483157396316528.\n",
      "[I 2024-05-07 21:28:44,354] Trial 5 finished with value: 0.950887143611908 and parameters: {'k_neighbors': 10, 'sampling_strategy': 'not majority', 'learning_rate': 3.510212330494174e-05, 'dropout_rate': 0.1056717953956326, 'batch_size': 256}. Best is trial 5 with value: 0.950887143611908.\n",
      "[I 2024-05-07 21:32:06,652] Trial 6 finished with value: 0.9349446892738342 and parameters: {'k_neighbors': 6, 'sampling_strategy': 'not majority', 'learning_rate': 5.1419074962829865e-05, 'dropout_rate': 0.0005372365763150698, 'batch_size': 128}. Best is trial 5 with value: 0.950887143611908.\n",
      "[I 2024-05-07 21:35:31,038] Trial 7 finished with value: 0.9519156813621521 and parameters: {'k_neighbors': 4, 'sampling_strategy': 'not majority', 'learning_rate': 0.00023492720872106624, 'dropout_rate': 0.6272185030745854, 'batch_size': 128}. Best is trial 7 with value: 0.9519156813621521.\n",
      "[I 2024-05-07 21:38:48,438] Trial 8 finished with value: 0.9470300674438477 and parameters: {'k_neighbors': 2, 'sampling_strategy': 'all', 'learning_rate': 0.0039946555598779545, 'dropout_rate': 0.6172540537466706, 'batch_size': 128}. Best is trial 7 with value: 0.9519156813621521.\n",
      "[I 2024-05-07 21:42:45,617] Trial 9 finished with value: 0.949344277381897 and parameters: {'k_neighbors': 5, 'sampling_strategy': 'all', 'learning_rate': 1.3649230481708149e-05, 'dropout_rate': 0.42886693102386264, 'batch_size': 64}. Best is trial 7 with value: 0.9519156813621521.\n",
      "[I 2024-05-07 21:45:04,456] Trial 10 finished with value: 0.9483157396316528 and parameters: {'k_neighbors': 3, 'sampling_strategy': 'minority', 'learning_rate': 0.0003618737735620397, 'dropout_rate': 0.3287821917992664, 'batch_size': 32}. Best is trial 7 with value: 0.9519156813621521.\n",
      "[I 2024-05-07 21:47:47,306] Trial 11 finished with value: 0.9480586051940918 and parameters: {'k_neighbors': 4, 'sampling_strategy': 'not majority', 'learning_rate': 9.396300120849925e-05, 'dropout_rate': 0.14774630034277433, 'batch_size': 256}. Best is trial 7 with value: 0.9519156813621521.\n",
      "[I 2024-05-07 21:50:21,511] Trial 12 finished with value: 0.9426587820053101 and parameters: {'k_neighbors': 7, 'sampling_strategy': 'not majority', 'learning_rate': 0.00011922125643682137, 'dropout_rate': 0.17740998350808712, 'batch_size': 256}. Best is trial 7 with value: 0.9519156813621521.\n"
     ]
    }
   ],
   "source": [
    "def build_and_train_model(X_train, y_train, X_test, y_test, trial):\n",
    "    # Параметры для SMOTE\n",
    "    k_neighbors = trial.suggest_int('k_neighbors', 2, 10)\n",
    "    sampling_strategy = trial.suggest_categorical('sampling_strategy', ['auto', 'minority', 'not majority', 'all'])\n",
    "    \n",
    "    # Применяем SMOTE\n",
    "    smote = SMOTE(k_neighbors=k_neighbors, sampling_strategy=sampling_strategy)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Параметры модели\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
    "    dropout_rate = trial.suggest_uniform('dropout_rate', 0.0, 0.7)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    epochs = 100\n",
    "\n",
    "    num_classes = len(np.unique(y_resampled))\n",
    "    \n",
    "    # Создание модели\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(X_resampled.shape[1],)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Компиляция модели\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Обучение модели\n",
    "    history = model.fit(X_resampled, y_resampled, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=0)\n",
    "    \n",
    "    # Оценка модели\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    return test_accuracy\n",
    "\n",
    "def objective(trial):\n",
    "    return build_and_train_model(X_train, y_train, X_test, y_test, trial)\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "print(\"Лучшие параметры:\")\n",
    "print(study.best_trial)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
